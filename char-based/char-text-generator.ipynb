{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Character-Based Text Generator\n",
    "\n",
    "This notebook will walk through the way that the <a href=\"https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py\">source project</a> wrote. Minor changes were added in the writing of this code in order to better suit our needs. \n",
    "\n",
    "One change was a change in the source material acting as the dataset. These models were inteded for use in an extension for MIT App Inventor, whose target audience is students in middle and high school. As such, we felt that children's books were more appropriate books to train the models on compared to the original dataset which were \"Nietzsche's writings.\"\n",
    "\n",
    "Another change was the ability to resume training a saved model. This was made mostly for convenience, in order to pause and resume training spontaneously.\n",
    "\n",
    "One change to consider is adding **dropout** to the models, and whether adding dropout is worth it if we intend to host a word-based text generator instead of these character based ones. While training, dropout helps to reducing overfitting. This is useful since some of the books in the dataset folder, such as Dr Seuss's texts, are not very long. Measures should to be taken in order to mitigate this risk of overfitting the training set data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "These models are trained using the Keras library. For the original device which trained these models, the backend was a gpu-enabled version of tensorflow, though to run the training, a handful of back-end deep-learning libraries are accepted by Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "# from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io, getopt, ast\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Here we will define some parameters for our model.\n",
    "\n",
    "The first parameter is the `dataset_path` which is simply the path to the text pile from which you want the model to train from. This model will then prepare this text file into training values.\n",
    "\n",
    "The next group of parameters is concerned with loading and saving the model. If you intend to load an existing model, you would set `load_file` to True and provide the location of the exported Keras model in the variable `load_path`. Regardless of whether you want to load the file, you must provide both the `save_path` and `file_name`. The save path is self-explanatory. `file_name` is the name of the file that you want to export the model to. **Notice**: `file_name` does not have a file extension. This is because the callback function which checkpoints the model will add to the end of the string the number of epochs elapsed and the extension name that Keras applies to the exported models automatically. \n",
    "\n",
    "The final group of parameters are the **hyperparameters** for the model. It determines the number of epochs to train the model for, the batch size, the `look_back`, and the `step_size`.\n",
    "\n",
    "When preparing the dataset, we split the text into a series of input and output sequences. The length of each input data is `look_back` characters long. The output is the character which follows those `look_back` characters. For example, in this case, the look back is defined to be 40 characters long. The model will take the first 40 characters, with indexes from 0-39 inclusive, and label them as the input sequence. The next character, at index 40, will then be labeled the output sequence. \n",
    "\n",
    "The model will then repeat this process for the characters in the range 0+`step_size` to 39+`step_size` inclusive. You may want to increase the step size if you have a particularly large dataset and don't have the memory available to separate the document into such a large dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./datasets/narnia-1.txt\"\n",
    "\n",
    "load_file = False\n",
    "load_path = \"./checkpoint.h5\"\n",
    "save_path = \"./\"\n",
    "file_name = \"narnia-1\"\n",
    "\n",
    "num_epochs = 5\n",
    "checkpoints = list(range(num_epochs))\n",
    "batch_size = 256\n",
    "look_back = 40\n",
    "step_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Processing\n",
    "\n",
    "This next block will be concerned with reading and preparing the text. We will be notified of the length of the body of text (corpus) and the total number of characters in the dataset.\n",
    "\n",
    "Additionally, because this code is inteded to be exported to tensorflow.js and hosted on a webserver, I read and exported the charset as a Javascript array. Using this, I would be able to decode the output of the model.\n",
    "\n",
    "Notice that this is done regardless of whether we are loading an existing model or building a new one. In the future, it may be desirable to also save the charset with a given model so as to improve replicability and save time when training an existing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.open(dataset_path, encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# Outputs a charset\n",
    "charset = sorted(list(set(text)))\n",
    "# Make char's JS Readable\n",
    "for i in range(len(charset)):\n",
    "    if (charset[i] == '\\n'):\n",
    "        charset[i] = '\\\\n'\n",
    "    if (charset[i] == '\"'):\n",
    "        charset[i] = '\\\\\"'\n",
    "# Generates the charset file\n",
    "f = open(file_name + \"-charset.txt\",\"w+\")\n",
    "charset_final = '[\"'+ '\", \"'.join(charset) + '\"]'\n",
    "f.write(charset_final)\n",
    "f.close()\n",
    "\n",
    "print (charset_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "Here, we will cut the text into semi-redundant sequences of characters with lenght `look_back` as described above. We will be notified of the number of sequences. \n",
    "\n",
    "Finally, we convert these characters into integers, according to their index in `charset`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = look_back\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step_size):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build or Load a Model\n",
    "\n",
    "Now, we will build the model. When loading the model, we assume that the model had built to except the specific input length and has the proper output length for training purposes. Otherwise we build a simple LSTM model with the given model summary. \n",
    "\n",
    "For the purposes of the project, we also save the model before it has done any training. This is for educational purposes, illustrating that the initialized model, not yet exposed to any training data, will essentially be completely random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = Path(load_path)\n",
    "if load_file and my_file.is_file():\n",
    "    print(\"Found Checkpoint. Loading saved model...\")\n",
    "    model = load_model(load_path)\n",
    "else:\n",
    "    if load_file:\n",
    "        print(\"Checkpoint not found. Building a new model instead.\")\n",
    "    print('Building model...')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "    model.add(Dense(len(chars), activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01))\n",
    "    model.save(\"{}-{}.h5\".format(file_name,0))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function and Callback Functions\n",
    "\n",
    "Here, we define a helper function and a callback function.\n",
    "\n",
    "The helper function, `sample` will, given the prediction of the mdoel with some probability, and a temperature which determines how likely we will select the most confident character. \n",
    "\n",
    "The function `on_epoch_end` is the callback function which runs every time the model has completed a single epoch. In this function, I added the ability to checkpoint the model at various stages of training. These various stages of training would help to illustrate to the middle and high school students that the models get better as they train for longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "\n",
    "    # Checkpointing the model\n",
    "    for i in checkpoints:\n",
    "        if epoch + 1 == i:\n",
    "            print(\"Checkpointing the model...\")\n",
    "            model.save(\"%s-%d.h5\" % (file_name,i))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    diversity = 1\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    print('----- diversity:', diversity)\n",
    "\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Finally, we begin the training. This is fairly self explanatory. The hyperparameters and training data are defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks if the checkpoints array says to checkpoint initial model\n",
    "if 0 in checkpoints:\n",
    "    print(\"Checkpointing the model...\")\n",
    "    model.save(\"%s-%d.h5\" % (file_name,0))\n",
    "\n",
    "# begin the training process    \n",
    "model.fit(x, y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=num_epochs,\n",
    "          callbacks=[print_callback])\n",
    "\n",
    "print(\"Source: \\\"%s\\\" \\nEpochs: %d \\nBatch Size: %d \\nStep Size: %d \\n Look Back: %d\" % (dataset_path, num_epochs, batch_size, step_size, maxlen))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work\n",
    "\n",
    "It may be worth implementing a dropout layer to the models.\n",
    "\n",
    "Among other features..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
