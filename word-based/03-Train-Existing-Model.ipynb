{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import string\n",
    "from numpy import array\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from pickle import load\n",
    "from pickle import dump\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from random import randint\n",
    "import sys\n",
    "import io, getopt, ast\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# parameters\n",
    "\n",
    "# Path to the raw corpus\n",
    "raw_corpus =\"../datasets/harry-potter-1.txt\"\n",
    "# This is the file with the pre separated lines of 51 words\n",
    "dataset_path = \"./harry-potter.txt\"\n",
    "\n",
    "load_existing_model = False\n",
    "filename  = \"generated-model\"\n",
    "load_path = \"./\" + filename + \".h5\"\n",
    "save_path = \"./\"\n",
    "\n",
    "num_epochs = 5\n",
    "checkpoints = list(range(1,num_epochs+1))\n",
    "batch_size = 256\n",
    "words_to_generate = 60\n",
    "\n",
    "input_size = 50\n",
    "output_size = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-model.py -d <dataset> -c <epochs to checkpoint> -e <# epochs> -l <model to load from> -n <name of files>\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomiyee/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3275: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# Takes Command line inputs to override the above\n",
    "if __name__ == \"__main__\":\n",
    "    argv = sys.argv[1:]\n",
    "\n",
    "    try:\n",
    "        opts, args = getopt.getopt(argv,\"hd:c:e:l:n:\",[\"dataset=\",\"checkpoints=\",\"epochs=\",\"load_model=\",\"name=\"])\n",
    "    except getopt.GetoptError:\n",
    "        print ('train-model.py -d <dataset> -c <epochs to checkpoint> -e <# epochs> -l <model to load from> -n <name of files>')\n",
    "        sys.exit(2)\n",
    "\n",
    "    for opt, arg in opts:\n",
    "\n",
    "        # Help Command\n",
    "        if opt == '-h':\n",
    "            print ('train-model.py -d <dataset> -c <epochs to checkpoint> -e <# epochs> -l <model to load from> -n <name of files>')\n",
    "            sys.exit()\n",
    "\n",
    "        # Num Epochs\n",
    "        elif opt in (\"-e\",\"--epochs\"):\n",
    "            num_epochs = ast.literal_eval(arg)\n",
    "            checkpoints = list(range(num_epochs))\n",
    "\n",
    "        # Load Model\n",
    "        elif opt in (\"-l\", \"--load_model\"):\n",
    "            load_existing_model = True\n",
    "            filename = arg\n",
    "            \n",
    "        # Name of the file with the raw corpus\n",
    "        elif opt in (\"-d\", \"--dataset\"):\n",
    "            raw_corpus = \"../datasets/\" + arg\n",
    "\n",
    "        # Checkpoints\n",
    "        elif opt in (\"-c\", \"--checkpoints\"):\n",
    "            checkpoints = ast.literal_eval(arg)\n",
    "\n",
    "        elif opt in (\"-n\", \"--name\"):\n",
    "            file_name = arg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating New Model...\n",
      "['mr', 'and', 'mrs', 'dursley', 'of', 'number', 'four', 'privet', 'drive', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', 'thank', 'you']\n",
      "Total Tokens: 77788\n",
      "Unique Tokens: 5902\n",
      "Total Sequences: 77737\n",
      "5903\n",
      "WARNING:tensorflow:From /home/tomiyee/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/tomiyee/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            295150    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 96)            56448     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 96)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 96)                74112     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 96)                9312      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5903)              572591    \n",
      "=================================================================\n",
      "Total params: 1,007,613\n",
      "Trainable params: 1,007,613\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# Load The Model or create a new model\n",
    "if load_existing_model:\n",
    "    print(\"Loading Existing Model...\")\n",
    "    # load the model\n",
    "    model = load_model(filename + '.h5')\n",
    "    # load the tokenizer\n",
    "    tokenizer = load(open(filename + '-tokenizer.pkl', 'rb'))\n",
    "else:\n",
    "    print(\"Generating New Model...\")\n",
    "    \n",
    "    # =================================================\n",
    "    # Dataset Acquisition\n",
    "    \n",
    "    # loads doc into memory\n",
    "    def load_doc(filename):\n",
    "        # open the file as read only\n",
    "        file = open(filename, 'r')\n",
    "        # read all text\n",
    "        text = file.read()\n",
    "        # close the file\n",
    "        file.close()\n",
    "        return text\n",
    "\n",
    "    # turns a doc into clean tokens\n",
    "    def clean_doc(doc):\n",
    "        # make lower case\n",
    "        doc = doc.lower()\n",
    "        # replace '--' with a space ' '\n",
    "        doc = doc.replace('--', ' ')\n",
    "        doc = doc.replace('-', ' ')\n",
    "        # split into tokens by white space\n",
    "        tokens = doc.split()\n",
    "        # remove punctuation from each token\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        return tokens\n",
    "\n",
    "    # load document\n",
    "    doc = load_doc(raw_corpus)\n",
    "    # clean document\n",
    "    tokens = clean_doc(doc)\n",
    "    print(tokens[:20])\n",
    "    print('Total Tokens: %d' % len(tokens))\n",
    "    print('Unique Tokens: %d' % len(set(tokens)))\n",
    "    \n",
    "    # =================================================\n",
    "    # Dataset Preparation and Preservation\n",
    "\n",
    "    # organize into sequences of tokens\n",
    "    length = input_size + output_size\n",
    "    sequences = list()\n",
    "    for i in range(length, len(tokens)):\n",
    "        # select sequence of tokens\n",
    "        seq = tokens[i-length:i]\n",
    "        # convert into a line\n",
    "        line = ' '.join(seq)\n",
    "        # store\n",
    "        sequences.append(line)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "    # save tokens to file, one dialog per line\n",
    "    def save_doc(lines, filename):\n",
    "        data = '\\n'.join(lines)\n",
    "        file = open(filename, 'w')\n",
    "        file.write(data)\n",
    "        file.close()\n",
    "\n",
    "    # save sequences to file\n",
    "    save_doc(sequences, filename + \"-lines.txt\")\n",
    "    \n",
    "    # =================================================\n",
    "    # Tokenize Lines, Vocab Size Determination\n",
    "    \n",
    "    # load doc into memory\n",
    "    def load_doc(filename):\n",
    "        # open the file as read only\n",
    "        file = open(filename, 'r')\n",
    "        # read all text\n",
    "        text = file.read()\n",
    "        # close the file\n",
    "        file.close()\n",
    "        return text\n",
    "\n",
    "    # load\n",
    "    doc = load_doc(filename + \"-lines.txt\")\n",
    "    lines = doc.split('\\n')\n",
    "    \n",
    "    # integer encode sequences of words\n",
    "    tokenizer = Tokenizer(filters=string.punctuation)\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    sequences = tokenizer.texts_to_sequences(lines)\n",
    "    # save the tokenizer\n",
    "    dump(tokenizer, open(filename + '-tokenizer.pkl', 'wb'))\n",
    "    \n",
    "    # vocabulary size\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print(vocab_size)\n",
    "    \n",
    "    # separate into input and output\n",
    "    sequences = array(sequences)\n",
    "    X, y = sequences[:,:-1], sequences[:,-1]\n",
    "    y = to_categorical(y, num_classes=vocab_size)\n",
    "    seq_length = X.shape[1]\n",
    "    \n",
    "    # =================================================\n",
    "    # Model Creation\n",
    "\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    from keras.layers import LSTM\n",
    "    from keras.layers import Embedding\n",
    "    from keras.layers import Dropout\n",
    "\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, input_size, input_length=seq_length))\n",
    "    model.add(LSTM(96, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(96))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(96, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Load the Dataset with the lines of text\n",
    "\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "doc = load_doc(filename + \"-lines.txt\")\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 5903\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# Use the tokenizer we just loaded to prepare the sequences we're using\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open(filename + '-tokenizer.pkl', 'rb'))\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "\n",
    "# remove punctuation from each token\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocab Size: %d\" % vocab_size)\n",
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Define function to define the generated text\n",
    "\n",
    "def generate_text():\n",
    "    \n",
    "    result = list()\n",
    "    # select a seed text\n",
    "    seed_text = lines[randint(0,len(lines))]\n",
    "    \n",
    "    for i in range(words_to_generate):\n",
    "        # encode the seed text\n",
    "        encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "\n",
    "        # append to input\n",
    "        seed_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Define the Callback Function\n",
    "\n",
    "def on_epoch_end (epoch, _):\n",
    "    \n",
    "    # Checkpointing the model\n",
    "    for i in checkpoints:\n",
    "        if epoch + 1 == i:\n",
    "            print(\"Checkpointing the model...\")\n",
    "            model.save(\"%s-cp-%d.h5\" % (filename, i))\n",
    "            break\n",
    "    print(\"Generating Text...\")\n",
    "    print(generate_text())\n",
    "    \n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "77737/77737 [==============================] - 22s 283us/step - loss: 6.2279 - acc: 0.0566\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "and was and and was and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and\n",
      "Epoch 2/5\n",
      "77737/77737 [==============================] - 22s 284us/step - loss: 6.1661 - acc: 0.0608\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "head and was was and and was was and and the head and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and\n",
      "Epoch 3/5\n",
      "77737/77737 [==============================] - 22s 283us/step - loss: 6.1211 - acc: 0.0632\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "was and and the other of and was and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and\n",
      "Epoch 4/5\n",
      "77737/77737 [==============================] - 22s 284us/step - loss: 6.0807 - acc: 0.0657\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "the other of and was and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and\n",
      "Epoch 5/5\n",
      "77737/77737 [==============================] - 22s 282us/step - loss: 6.0422 - acc: 0.0681\n",
      "Generating Text...\n",
      "was a door and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f309d9721d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# Fit Model\n",
    "\n",
    "model.fit(X, y, batch_size=batch_size, epochs=num_epochs, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "77737/77737 [==============================] - 22s 283us/step - loss: 6.0047 - acc: 0.0704\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and\n",
      "Epoch 2/40\n",
      "77737/77737 [==============================] - 22s 283us/step - loss: 5.9620 - acc: 0.0749\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "a head and the head and the head and the head and the a head and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and\n",
      "Epoch 3/40\n",
      "77737/77737 [==============================] - 22s 282us/step - loss: 5.9264 - acc: 0.0791\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "the head and the head and the head and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and\n",
      "Epoch 4/40\n",
      "77737/77737 [==============================] - 22s 283us/step - loss: 5.8890 - acc: 0.0829\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "the head and the head and the head and the head and the head and the head and the head and the a head and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and\n",
      "Epoch 5/40\n",
      "77737/77737 [==============================] - 22s 283us/step - loss: 5.8530 - acc: 0.0853\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "a the room and the head and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and\n",
      "Epoch 6/40\n",
      "77737/77737 [==============================] - 22s 282us/step - loss: 5.8205 - acc: 0.0863\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "was a the room and the door and the the room and the a way of the door and the the room and the the room and the a way of the the room and the way of the the room and the way of the the room and the way of the the room and the way of the\n",
      "Epoch 7/40\n",
      "77737/77737 [==============================] - 22s 283us/step - loss: 5.7877 - acc: 0.0891\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "and the the room and the way and the the room and the way and the door and the the room and the way and the the room and the way and the door and the the room and the way and the the room and the way and the door and the the room and the way and the\n",
      "Epoch 8/40\n",
      "77737/77737 [==============================] - 22s 283us/step - loss: 5.7623 - acc: 0.0911\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "a the room and the way and the the of and the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the\n",
      "Epoch 9/40\n",
      "77737/77737 [==============================] - 22s 283us/step - loss: 5.7322 - acc: 0.0932\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "he was a the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the\n",
      "Epoch 10/40\n",
      "77737/77737 [==============================] - 22s 282us/step - loss: 5.7107 - acc: 0.0951\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "and the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the\n",
      "Epoch 11/40\n",
      "77737/77737 [==============================] - 22s 282us/step - loss: 5.6874 - acc: 0.0964\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "door and the door and the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the\n",
      "Epoch 12/40\n",
      "77737/77737 [==============================] - 22s 283us/step - loss: 5.6620 - acc: 0.0984\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "and the the and and the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the\n",
      "Epoch 13/40\n",
      "77737/77737 [==============================] - 22s 283us/step - loss: 5.6390 - acc: 0.0991\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "the door and the the and and the of the the and and the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of the the of\n",
      "Epoch 14/40\n",
      "77737/77737 [==============================] - 22s 283us/step - loss: 5.6196 - acc: 0.1012\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "got to be a the and and the and and and and and and and the and and and and the and and and and and the and and and and and the and and and and and the and and and and and the and and and and and the and and and and and the and and and\n",
      "Epoch 15/40\n",
      "77737/77737 [==============================] - 22s 283us/step - loss: 5.5998 - acc: 0.1036\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "was a the and and the and and and and and and and and and the and and and the and and and and and and and and and the and and and the and and and and and and and and and and the and and and the and and and and and and and and and and the\n",
      "Epoch 16/40\n",
      "77737/77737 [==============================] - 22s 282us/step - loss: 5.5806 - acc: 0.1047\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "the and and the and and the and and and the and and and and and and the and and and and and and and the and and and and and and the and and and and and and the and and and and and and the and and and and and and the and and and and and and\n",
      "Epoch 17/40\n",
      "77737/77737 [==============================] - 22s 283us/step - loss: 5.5565 - acc: 0.1055\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "bit of the and and the and and the and and and and and and and the and and the and and and the and and and and and and the and and the and and and and and and the and and the and and and and and and the and and the and and and and and and\n",
      "Epoch 18/40\n",
      "77737/77737 [==============================] - 22s 282us/step - loss: 5.5416 - acc: 0.1077\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "and harry was a bit of the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and\n",
      "Epoch 19/40\n",
      "77737/77737 [==============================] - 22s 282us/step - loss: 5.5243 - acc: 0.1086\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "head and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the\n",
      "Epoch 20/40\n",
      "77737/77737 [==============================] - 22s 283us/step - loss: 5.5089 - acc: 0.1100\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "head and was a the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and\n",
      "Epoch 21/40\n",
      "77737/77737 [==============================] - 22s 283us/step - loss: 5.4902 - acc: 0.1109\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "was a very and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the and and the\n",
      "Epoch 22/40\n",
      "77737/77737 [==============================] - 22s 282us/step - loss: 5.4769 - acc: 0.1129\n",
      "Checkpointing the model...\n",
      "Generating Text...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the and the and the very very and and the and the and the very very and and the and the and the very very and and the and the and the very very and and the and the and the very very and and the and the and the very very and and the and the and the very\n",
      "Epoch 23/40\n",
      "77737/77737 [==============================] - 22s 282us/step - loss: 5.4580 - acc: 0.1140\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "he was the and the and the and the and and the and the and the and the very and and the and the and the and the very and and the and the and the and the very and and the and the and the and the very and and the and the and the and the very and\n",
      "Epoch 24/40\n",
      "77737/77737 [==============================] - 22s 282us/step - loss: 5.4446 - acc: 0.1145\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "and was the and the and the and and the and the and and the and the very and and the and the very and and the and the and and the and the very and and the and the very and and the and the and and the and the very and and the and the very and and\n",
      "Epoch 25/40\n",
      "77737/77737 [==============================] - 22s 282us/step - loss: 5.4343 - acc: 0.1154\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "very and and the very and and the and the very very the very very and and the very and and the and the very and and the very and and the and the very and and the very and and the and the very and and the very and and the and the very and and the very and\n",
      "Epoch 26/40\n",
      "77737/77737 [==============================] - 22s 282us/step - loss: 5.4217 - acc: 0.1164\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "was a very the and the very the and the very and the very and the very the and the very and the very and the very and the very and the very and the very and the very and and the very and and the very and and the very and and the very the very very very the\n",
      "Epoch 27/40\n",
      "77737/77737 [==============================] - 22s 282us/step - loss: 5.4072 - acc: 0.1178\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "very and and the very and and the very the and and the very and and the and the very and and the and the very and and the very and and the and the very and and the very and and the and the very and and the very and and the and the very and and the very\n",
      "Epoch 28/40\n",
      "77737/77737 [==============================] - 22s 283us/step - loss: 5.3972 - acc: 0.1188\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "at the door and the the and the very the and the very the and the very the and the very the and the very and the very the and the very and the very the and the very and the very the and the and the and the and the and the and the very the and the very\n",
      "Epoch 29/40\n",
      "77737/77737 [==============================] - 22s 282us/step - loss: 5.3824 - acc: 0.1197\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "it was a very the very very the very very very the very very the very very the very very the very very the very very the very very the very very the very very the and and the very the very very very the and and the back and the very very very and and the first hall and\n",
      "Epoch 30/40\n",
      "77737/77737 [==============================] - 22s 282us/step - loss: 5.3748 - acc: 0.1206\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "at the very very the very very very the very very the very very the very very the very very the very very the very very the very very the very very the stone and the and the very very very the stone was the the and the very very very the very very the stone and the and the\n",
      "Epoch 31/40\n",
      "77737/77737 [==============================] - 22s 282us/step - loss: 5.3625 - acc: 0.1208\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "the the very the very very the very and and the the and the very very very the very very the very very the very very the very very the very very and the very very the very very the very very the stone was a the and the the and the and the and the and the and the\n",
      "Epoch 32/40\n",
      "77737/77737 [==============================] - 22s 282us/step - loss: 5.3546 - acc: 0.1211\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "the the and and the very very very very a very very the very very the very very the very very the very very the very very very the very very the stone was the the and the very very very very the stone was a the and the and the and and was a the and the very the\n",
      "Epoch 33/40\n",
      "77737/77737 [==============================] - 22s 282us/step - loss: 5.3395 - acc: 0.1218\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "at the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the\n",
      "Epoch 34/40\n",
      "77737/77737 [==============================] - 22s 282us/step - loss: 5.3142 - acc: 0.1233\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "at the door and the few of the the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the\n",
      "Epoch 35/40\n",
      "77737/77737 [==============================] - 22s 282us/step - loss: 5.2965 - acc: 0.1242\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "stone had been the the the and and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the\n",
      "Epoch 36/40\n",
      "77737/77737 [==============================] - 22s 283us/step - loss: 5.2821 - acc: 0.1238\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "of the the and the stone and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the stone the stone the the and the\n",
      "Epoch 37/40\n",
      "77737/77737 [==============================] - 22s 282us/step - loss: 5.2713 - acc: 0.1254\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "to be the the and the the and the the stone the the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and the the and\n",
      "Epoch 38/40\n",
      "77737/77737 [==============================] - 22s 282us/step - loss: 5.2598 - acc: 0.1256\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "harry said harry and the bit and the the the and the the stone the the stone the the stone the the stone the the stone the the stone the few hall had the the the and the few hall had the the the and the the stone the few hall had the the the and the the stone the\n",
      "Epoch 39/40\n",
      "77737/77737 [==============================] - 22s 282us/step - loss: 5.2471 - acc: 0.1268\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "the bit of the the the and the the and the the the and the the and the the the and the the and the the the and the the and the the and the the and the the and the the the and the the and the the the and the the and the the the and the the\n",
      "Epoch 40/40\n",
      "77737/77737 [==============================] - 22s 282us/step - loss: 5.2398 - acc: 0.1275\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "going to be the the the and the the stone the the stone the the stone the the stone the the stone the the stone the the stone the the stone the the stone the the stone the the stone the the stone the the stone the the stone the few hall had the the and the few hall was\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f309d972320>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoints = list(range(1,41))\n",
    "model.fit(X, y, batch_size=batch_size, epochs=40, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
