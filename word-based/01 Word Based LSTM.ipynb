{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-Level Text Generator in Keras\n",
    "\n",
    "In this project, I will develop a word level text generator in Keras using LSTM. I will train it on a Harry Potter dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from numpy import array\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Document\n",
    "\n",
    "The first step in creating the model is to load the corpus into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've got the function to load the document, we find the path to the corpus that we are using, which in this case is:\n",
    "\n",
    "`../datasets/harry-potter-1.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter and the Sorcerer's Stone  CHAPTER ONE  THE BOY WHO LIVED  Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They \n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "in_filename = '../datasets/harry-potter-1.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Document\n",
    "\n",
    "Now that we've loaded the document into memory, we want to clean the document. For example, before splitting the document into words, we may want to replace all \"-\" with spaces to so that the words split more nicely. We also take out the punctuation from each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # make lower case\n",
    "    doc = doc.lower()\n",
    "    # replace '--' with a space ' '\n",
    "    doc = doc.replace('--', ' ')\n",
    "    doc = doc.replace('-', ' ')\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then run the cleaning function on the document that we've stored in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['harry', 'potter', 'and', 'the', 'sorcerers', 'stone', 'chapter', 'one', 'the', 'boy', 'who', 'lived', 'mr', 'and', 'mrs', 'dursley', 'of', 'number', 'four', 'privet']\n",
      "Total Tokens: 77888\n",
      "Unique Tokens: 5904\n"
     ]
    }
   ],
   "source": [
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:20])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "Now that we've tokenized our data (that is, separate the document into the list of words), we can now organize the dataset into input and output words. In this case, we've set the input to be 50 words followed by the next word. In otherwords, sequences of length 51, or `input_size + output_size`.\n",
    "\n",
    "The resulting list, `sequences`, is a list of strings with only 51 words each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 77837\n"
     ]
    }
   ],
   "source": [
    "input_size = 50\n",
    "output_size = 1\n",
    "# organize into sequences of tokens\n",
    "length = input_size + output_size\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we also create a function that will export the above list of strings to a separate document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sequences to file\n",
    "out_filename = 'harry-potter.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The Model\n",
    "\n",
    "Now that we've done the data preparation, we can load the dataset and really prepare it for the model by one hot encoding.\n",
    "\n",
    "## Load the Sequences\n",
    "\n",
    "First, we load the file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load\n",
    "in_filename = 'harry-potter.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the Sequences\n",
    "\n",
    "Now, before we can train the model on the data, we need to tokenize the data again. We load the Tokenizer, and use it to prepare the lines. We save these tokenized sentences into an array of sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a quick look at the vocabulary size of the model! We also store this vocabulary size into a variable `vocab_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5905\n"
     ]
    }
   ],
   "source": [
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've encoded the data, we need to separate the dataset into input `X` and output `y` elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Model\n",
    "\n",
    "Now, we get to the meat of things. We are going to define the model that we are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            295250    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5905)              596405    \n",
      "=================================================================\n",
      "Total params: 1,042,555\n",
      "Trainable params: 1,042,555\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, input_size, input_length=seq_length))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the model is ready to be fit on the data for some amount of epochs. This takes a few hours even on modern hardware without gpu's. You can speed up the training by increasing the `batch_size` or decreasing the number of `epochs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "77837/77837 [==============================] - 170s 2ms/step - loss: 6.6985 - acc: 0.0464\n",
      "Epoch 2/60\n",
      "77837/77837 [==============================] - 184s 2ms/step - loss: 6.3821 - acc: 0.0516\n",
      "Epoch 3/60\n",
      "77837/77837 [==============================] - 195s 3ms/step - loss: 6.1895 - acc: 0.0612\n",
      "Epoch 4/60\n",
      "77837/77837 [==============================] - 189s 2ms/step - loss: 6.0890 - acc: 0.0658\n",
      "Epoch 5/60\n",
      "77837/77837 [==============================] - 176s 2ms/step - loss: 6.0176 - acc: 0.0674\n",
      "Epoch 6/60\n",
      "77837/77837 [==============================] - 169s 2ms/step - loss: 5.9490 - acc: 0.0708\n",
      "Epoch 7/60\n",
      "77837/77837 [==============================] - 172s 2ms/step - loss: 5.8886 - acc: 0.0745\n",
      "Epoch 8/60\n",
      "77837/77837 [==============================] - 177s 2ms/step - loss: 5.8310 - acc: 0.0789\n",
      "Epoch 9/60\n",
      "77837/77837 [==============================] - 185s 2ms/step - loss: 5.7764 - acc: 0.0825\n",
      "Epoch 10/60\n",
      "77837/77837 [==============================] - 193s 2ms/step - loss: 5.7244 - acc: 0.0867\n",
      "Epoch 11/60\n",
      "77837/77837 [==============================] - 192s 2ms/step - loss: 5.6740 - acc: 0.0904\n",
      "Epoch 12/60\n",
      "77837/77837 [==============================] - 195s 3ms/step - loss: 5.6244 - acc: 0.0931\n",
      "Epoch 13/60\n",
      "77837/77837 [==============================] - 199s 3ms/step - loss: 5.5788 - acc: 0.0970\n",
      "Epoch 14/60\n",
      "77837/77837 [==============================] - 198s 3ms/step - loss: 5.5334 - acc: 0.1000\n",
      "Epoch 15/60\n",
      "77837/77837 [==============================] - 196s 3ms/step - loss: 5.4919 - acc: 0.1026\n",
      "Epoch 16/60\n",
      "77837/77837 [==============================] - 196s 3ms/step - loss: 5.4523 - acc: 0.1051\n",
      "Epoch 17/60\n",
      "77837/77837 [==============================] - 193s 2ms/step - loss: 5.4153 - acc: 0.1070\n",
      "Epoch 18/60\n",
      "77837/77837 [==============================] - 186s 2ms/step - loss: 5.3811 - acc: 0.1088\n",
      "Epoch 19/60\n",
      "77837/77837 [==============================] - 187s 2ms/step - loss: 5.3502 - acc: 0.1108\n",
      "Epoch 20/60\n",
      "77837/77837 [==============================] - 181s 2ms/step - loss: 5.3189 - acc: 0.1123\n",
      "Epoch 21/60\n",
      "77837/77837 [==============================] - 180s 2ms/step - loss: 5.2886 - acc: 0.1143\n",
      "Epoch 22/60\n",
      "77837/77837 [==============================] - 180s 2ms/step - loss: 5.2616 - acc: 0.1157\n",
      "Epoch 23/60\n",
      "77837/77837 [==============================] - 181s 2ms/step - loss: 5.2353 - acc: 0.1176\n",
      "Epoch 24/60\n",
      "77837/77837 [==============================] - 178s 2ms/step - loss: 5.2091 - acc: 0.1197\n",
      "Epoch 25/60\n",
      "77837/77837 [==============================] - 176s 2ms/step - loss: 5.1851 - acc: 0.1204\n",
      "Epoch 26/60\n",
      "77837/77837 [==============================] - 178s 2ms/step - loss: 5.1599 - acc: 0.1230\n",
      "Epoch 27/60\n",
      "77837/77837 [==============================] - 176s 2ms/step - loss: 5.1352 - acc: 0.1247\n",
      "Epoch 28/60\n",
      "77837/77837 [==============================] - 176s 2ms/step - loss: 5.1122 - acc: 0.1260\n",
      "Epoch 29/60\n",
      "77837/77837 [==============================] - 176s 2ms/step - loss: 5.0868 - acc: 0.1282\n",
      "Epoch 30/60\n",
      "77837/77837 [==============================] - 175s 2ms/step - loss: 5.0655 - acc: 0.1299\n",
      "Epoch 31/60\n",
      "77837/77837 [==============================] - 176s 2ms/step - loss: 5.0416 - acc: 0.1321\n",
      "Epoch 32/60\n",
      "77837/77837 [==============================] - 175s 2ms/step - loss: 5.0196 - acc: 0.1326\n",
      "Epoch 33/60\n",
      "77837/77837 [==============================] - 176s 2ms/step - loss: 4.9963 - acc: 0.1342\n",
      "Epoch 34/60\n",
      "77837/77837 [==============================] - 174s 2ms/step - loss: 4.9745 - acc: 0.1358\n",
      "Epoch 35/60\n",
      "77837/77837 [==============================] - 174s 2ms/step - loss: 4.9508 - acc: 0.1378\n",
      "Epoch 36/60\n",
      "77837/77837 [==============================] - 176s 2ms/step - loss: 4.9286 - acc: 0.1400\n",
      "Epoch 37/60\n",
      "77837/77837 [==============================] - 176s 2ms/step - loss: 4.9047 - acc: 0.1423\n",
      "Epoch 38/60\n",
      "77837/77837 [==============================] - 175s 2ms/step - loss: 4.8838 - acc: 0.1440\n",
      "Epoch 39/60\n",
      "77837/77837 [==============================] - 175s 2ms/step - loss: 4.8621 - acc: 0.1459\n",
      "Epoch 40/60\n",
      "77837/77837 [==============================] - 175s 2ms/step - loss: 4.8408 - acc: 0.1477\n",
      "Epoch 41/60\n",
      "77837/77837 [==============================] - 176s 2ms/step - loss: 4.8184 - acc: 0.1499\n",
      "Epoch 42/60\n",
      "77837/77837 [==============================] - 174s 2ms/step - loss: 4.7981 - acc: 0.1509\n",
      "Epoch 43/60\n",
      "77837/77837 [==============================] - 176s 2ms/step - loss: 4.7765 - acc: 0.1530\n",
      "Epoch 44/60\n",
      "77837/77837 [==============================] - 175s 2ms/step - loss: 4.7536 - acc: 0.1555\n",
      "Epoch 45/60\n",
      "77837/77837 [==============================] - 174s 2ms/step - loss: 4.7340 - acc: 0.1573\n",
      "Epoch 46/60\n",
      "77837/77837 [==============================] - 177s 2ms/step - loss: 4.7132 - acc: 0.1599\n",
      "Epoch 47/60\n",
      "77837/77837 [==============================] - 176s 2ms/step - loss: 4.6894 - acc: 0.1621\n",
      "Epoch 48/60\n",
      "77837/77837 [==============================] - 174s 2ms/step - loss: 4.6693 - acc: 0.1635\n",
      "Epoch 49/60\n",
      "77837/77837 [==============================] - 174s 2ms/step - loss: 4.6459 - acc: 0.1664\n",
      "Epoch 50/60\n",
      "77837/77837 [==============================] - 176s 2ms/step - loss: 4.6245 - acc: 0.1686\n",
      "Epoch 51/60\n",
      "77837/77837 [==============================] - 175s 2ms/step - loss: 4.6006 - acc: 0.1713\n",
      "Epoch 52/60\n",
      "77837/77837 [==============================] - 174s 2ms/step - loss: 4.5781 - acc: 0.1737\n",
      "Epoch 53/60\n",
      "77837/77837 [==============================] - 175s 2ms/step - loss: 4.5546 - acc: 0.1757\n",
      "Epoch 54/60\n",
      "77837/77837 [==============================] - 177s 2ms/step - loss: 4.5274 - acc: 0.1781\n",
      "Epoch 55/60\n",
      "77837/77837 [==============================] - 182s 2ms/step - loss: 4.5039 - acc: 0.1805\n",
      "Epoch 56/60\n",
      "77837/77837 [==============================] - 174s 2ms/step - loss: 4.4822 - acc: 0.1837\n",
      "Epoch 57/60\n",
      "77837/77837 [==============================] - 178s 2ms/step - loss: 4.4634 - acc: 0.1879\n",
      "Epoch 58/60\n",
      "77837/77837 [==============================] - 175s 2ms/step - loss: 4.4488 - acc: 0.1907\n",
      "Epoch 59/60\n",
      "77837/77837 [==============================] - 175s 2ms/step - loss: 4.4324 - acc: 0.1938\n",
      "Epoch 60/60\n",
      "77837/77837 [==============================] - 177s 2ms/step - loss: 4.4165 - acc: 0.1960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12b897c18>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=256, epochs=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then also save this model. This is the end of the tutorial linked above, but personally, I prefer to save my progress while training and not at the very end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end (epoch, _):\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(\"Checkpointing the model...\")\n",
    "        model.save(\"%s.h5\" % (\"harry-potter\"))\n",
    "        \n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " - 154s - loss: 3.2023 - acc: 0.3784\n",
      "Epoch 2/20\n",
      " - 151s - loss: 3.1868 - acc: 0.3819\n",
      "Epoch 3/20\n",
      " - 151s - loss: 3.1741 - acc: 0.3848\n",
      "Epoch 4/20\n",
      " - 153s - loss: 3.1625 - acc: 0.3867\n",
      "Epoch 5/20\n",
      " - 154s - loss: 3.1501 - acc: 0.3881\n",
      "Checkpointing the model...\n",
      "Epoch 6/20\n",
      " - 152s - loss: 3.1364 - acc: 0.3897\n",
      "Epoch 7/20\n",
      " - 167s - loss: 3.1270 - acc: 0.3929\n",
      "Epoch 8/20\n",
      " - 161s - loss: 3.1150 - acc: 0.3939\n",
      "Epoch 9/20\n",
      " - 165s - loss: 3.1006 - acc: 0.3955\n",
      "Epoch 10/20\n",
      " - 156s - loss: 3.0897 - acc: 0.3978\n",
      "Checkpointing the model...\n",
      "Epoch 11/20\n",
      " - 154s - loss: 3.0773 - acc: 0.3990\n",
      "Epoch 12/20\n",
      " - 462s - loss: 3.0671 - acc: 0.4028\n",
      "Epoch 13/20\n",
      " - 179s - loss: 3.0573 - acc: 0.4029\n",
      "Epoch 14/20\n",
      " - 153s - loss: 3.0460 - acc: 0.4042\n",
      "Epoch 15/20\n",
      " - 154s - loss: 3.0355 - acc: 0.4078\n",
      "Checkpointing the model...\n",
      "Epoch 16/20\n",
      " - 153s - loss: 3.0246 - acc: 0.4083\n",
      "Epoch 17/20\n",
      " - 158s - loss: 3.0108 - acc: 0.4108\n",
      "Epoch 18/20\n",
      " - 154s - loss: 3.0044 - acc: 0.4117\n",
      "Epoch 19/20\n",
      " - 162s - loss: 2.9962 - acc: 0.4133\n",
      "Epoch 20/20\n",
      " - 152s - loss: 2.9847 - acc: 0.4152\n",
      "Checkpointing the model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12b948518>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, batch_size=256, epochs=20, verbose=2, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Models and the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# load cleaned text sequences\n",
    "in_filename = 'harry-potter.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "seq_length = len(lines[0].split()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from pickle import load\n",
    "\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arms around the trolls neck from behind the troll couldnt feel harry hanging there but even a troll will notice if you stick a long bit of wood up its nose and harrys wand had still been in his hand when hed jumped it had gone straight up one of the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[456, 80, 1, 1218, 574, 41, 150, 1, 371, 109, 530, 7, 800, 35, 22, 108, 4, 371, 136, 1132, 40, 12, 995, 4, 172, 181, 6, 228, 27, 44, 365, 2, 98, 195, 14, 130, 52, 10, 11, 178, 66, 83, 744, 9, 14, 235, 424, 27, 38, 6, 1]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate sequences to a fixed length\n",
    "encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "# predict probabilities for each word\n",
    "yhat = model.predict_classes(encoded, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map predicted word index to word\n",
    "out_word = ''\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index == yhat:\n",
    "        out_word = word\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-3565b389f745>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# append to input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mseed_text\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mout_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# append to input\n",
    "seed_text += ' ' + out_word\n",
    "result.append(out_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "# load cleaned text sequences\n",
    "in_filename = 'harry-potter.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
