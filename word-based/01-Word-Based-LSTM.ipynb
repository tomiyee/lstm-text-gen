{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-Based Text Generator in Keras\n",
    "\n",
    "This notebook will describe a word-based text generator outlined in <a href='https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/'>this tutorial</a> written by Jason Brownlee. Similarly to the Char-Based text generater, this model is intended to be exported to tensorflow.js and hosted on a web server for MIT App Inventor in order to teach middle and high school students the fundamental concepts for AI and Machine Learning. \n",
    "\n",
    "In this document, I have combine three almost independent parts of the process of training this word-based text generator. These parts are:\n",
    "* Process the corpus\n",
    "* Train the Model\n",
    "* Use the Model\n",
    "\n",
    "Basically, you can process the corpus with one python script, train a new (or existing) model with another, and generate text from an exported model with a third python script.\n",
    "\n",
    "This should provide some context for a little bit of redundancy in this code. An example is saving and loading text to a file when you could simply continue using the variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Parameters\n",
    "\n",
    "This model is also trained using the Keras library. \n",
    "\n",
    "The parameters are similar to the ones used in the Char-Based model. Here, the variable `in_filename` represents the path to the body of text (corpus) on which we will be training the model. The way we will be processing the corpus, described in a later section, we also need an `out_filename` which will be the new text file. The constant `TEXT_NAME` will be the name of the file that we will be using to save the model, its tokenizer, and its processed out_file.\n",
    "\n",
    "The `input_size` is the same as the `look_back` in the Char-Based generator. Previously, the Char-Based generator had assumed that the output_size of the model would just be 1. In this project, there is now an explicit variable stating that the expected output_size would be 1. I'm not sure what would happen if we increased the output_size. It may or may not create a model that produces the next two words, or it may break...\n",
    "\n",
    "For the sake of this example, we will be training using the Dr. Seuss text file saved in the datasets folder. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from numpy import array\n",
    "from random import randint\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "from pickle import dump\n",
    "\n",
    "in_filename = '../datasets/drseuss.txt'\n",
    "TEXT_NAME = 'drseuss'\n",
    "out_filename = '{}-lines.txt'.format(TEXT_NAME)\n",
    "\n",
    "input_size = 50\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtered Punctuation\n",
    "\n",
    "One parameter that I added is `filtered_punctuation` which is a string containing all of the punctuation which we will want to remove from the text. This cleans up the text significantly and reduces the vocabulary that the model may need to learn. \n",
    "\n",
    "After some training, I noticed that the models were not generating complete thoughts and seemed to get caught in run-on sentences. This makes sense, since the model never knew where to complete a thought and move onto the next one. As a result, I decided to remove commas and periods from the list of filtered_punctuation. My reasoning was that if I had the models consider periods and commas as words of their own, the resulting models would be able to generate more coherent thoughts. Some post processing will need to be applied to the resulting model's outputs in order to remove the space before the commas and periods, but aside from that, the resulting models had a significant improvement in coherency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_punctuation = string.punctuation.replace(',','').replace('.','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Document\n",
    "\n",
    "Here, Brownlee defines a helper funciton `load_doc` that will load the source text from the provided file path. It will then return the entire document as one large string. We can then take a look at the first handful of characters in our document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load document\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Document\n",
    "\n",
    "Now that we've loaded the document into memory, we want to clean and process the document. \n",
    "\n",
    "For example, before splitting the document into words, we may want to replace all \"-\" with spaces to so that the words split more nicely. \n",
    "\n",
    "We also remove the punctuation marks from the text, excluding those we mentioned earlier. \n",
    "\n",
    "We can then see the \"tokens\" in the document, which are simply the words in the text. In our case, this also includes any "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # make lower case\n",
    "    doc = doc.lower()\n",
    "    # replace '--' with a space ' '\n",
    "    doc = doc.replace('--', ' ')\n",
    "    doc = doc.replace('-', ' ')\n",
    "    # I put a space before the punctuation so that words like \"however,\" \n",
    "    # are not treated differently from ones like \"however \" due to the comma\n",
    "    doc = doc.replace('.', ' .')\n",
    "    doc = doc.replace(',', ' ,')\n",
    "    doc = doc.replace('  ', ' ')\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', filtered_punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha() or word == ',' or word == '.']\n",
    "    return tokens\n",
    "\n",
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:20])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "Now that we've tokenized our data (that is, separate the document into the list of words), we can now organize the dataset into input and output words. In this case, we've set the input to be 50 words followed by the next word. In otherwords, sequences of length 51, or `input_size + output_size`.\n",
    "\n",
    "The resulting list, `sequences`, is a list of strings with only 51 words each.\n",
    "\n",
    "Once we have finished creating this list of sequences, we want to save this to a different file with the name `out_filename`. This will become the actual training data for our new model once we create it. The helper function `save_doc` will help us do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 13783\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "length = input_size + output_size\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "# save sequences to file\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The Model\n",
    "\n",
    "From this point on, we can almost separate this code from what came above. Now that we've done the data preparation, we can load the dataset and really prepare it for the model by one hot encoding.\n",
    "\n",
    "## Load the Sequences\n",
    "\n",
    "Here, Brownlee defined a helper function `load_doc`. Given the file path to the document with the lines of length `input_size + output_size`, it will return the full document to you, after which you would need to split the massive string into an array of lines. \n",
    "\n",
    "Now we should have an arry called `lines` where every element is one string with 51 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat in the hat by dr seuss the sun did not shine it was too wet to play so we sat in the house all that cold cold wet day i sat there with sally we sat there we two and i said how i wish we had something to'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load\n",
    "doc = load_doc(out_filename)\n",
    "lines = doc.split('\\n')\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the Sequences\n",
    "\n",
    "Before we can train the model on the data, we need to tokenize the data again. We load the Tokenizer, and use it to prepare the lines. We save these tokenized sentences into an array of sequences. \n",
    "\n",
    "Finally, we export this tokenizer as a pkl, which we can load back into a different python script should we so choose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer(filters=filtered_punctuation)\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open(TEXT_NAME + '-tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the vocabulary size of the model. This basically shows how many unique words the model has available to choose from when predicting the next word.\n",
    "\n",
    "We'll also store this vocabulary size into the variable `vocab_size` for defining our model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1870\n"
     ]
    }
   ],
   "source": [
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've encoded the data, we need to separate the dataset into input `X` and output `y` elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Model\n",
    "\n",
    "Now, we get to the meat of things. We are going to define the model that we are going to use. The model that I plan on using will have the following architecture:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 50, 50)            93500     \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1870)              188870    \n",
      "=================================================================\n",
      "Total params: 433,270\n",
      "Trainable params: 433,270\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, input_size, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function and Callbacks\n",
    "\n",
    "Finally, the model is ready to be fit on the data for some amount of epochs. This takes quite a long time. You can speed up the training by increasing the `batch_size` or decreasing the number of `epochs`.\n",
    "\n",
    "We then also save this model. This is the point where the tutorial that I've been following ends, but personally I like to save my models periodically, so before we train, I'm going to define a callback function which will checkpoint the model every five epochs in case I get impatient. This utilizes the keras `LambdaCallback` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Define function to define the generated text\n",
    "\n",
    "SAVE_FILE_NAME = TEXT_NAME + \"-{}.h5\"\n",
    "\n",
    "def generate_text(words_to_generate=80):\n",
    "    \n",
    "    result = list()\n",
    "    # select a seed text\n",
    "    seed_text = lines[randint(0,len(lines))]\n",
    "    \n",
    "    for i in range(words_to_generate):\n",
    "        # encode the seed text\n",
    "        encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "\n",
    "        # append to input\n",
    "        seed_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "def on_epoch_end (epoch, _):\n",
    "    if (epoch + 1) % 25 == 0:\n",
    "        print(\"Checkpointing the model...\")\n",
    "        model.save(SAVE_FILE_NAME.format(epoch+1))\n",
    "        # save the tokenizer\n",
    "        dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "        print(\"Generating Text...\")\n",
    "        print(generate_text())\n",
    "        \n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(SAVE_FILE_NAME.format(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "13783/13783 [==============================] - 10s 697us/step - loss: 6.3297 - acc: 0.0384\n",
      "Epoch 2/500\n",
      "13783/13783 [==============================] - 8s 616us/step - loss: 6.0713 - acc: 0.0414\n",
      "Epoch 3/500\n",
      "13783/13783 [==============================] - 8s 616us/step - loss: 6.0146 - acc: 0.0426\n",
      "Epoch 4/500\n",
      "13783/13783 [==============================] - 8s 616us/step - loss: 5.9394 - acc: 0.0436\n",
      "Epoch 5/500\n",
      "13783/13783 [==============================] - 8s 614us/step - loss: 5.8626 - acc: 0.0484\n",
      "Epoch 6/500\n",
      "13783/13783 [==============================] - 8s 616us/step - loss: 5.7972 - acc: 0.0483\n",
      "Epoch 7/500\n",
      "13783/13783 [==============================] - 8s 616us/step - loss: 5.7424 - acc: 0.0474\n",
      "Epoch 8/500\n",
      "13783/13783 [==============================] - 8s 615us/step - loss: 5.6672 - acc: 0.0487\n",
      "Epoch 9/500\n",
      "13783/13783 [==============================] - 8s 616us/step - loss: 5.5961 - acc: 0.0496\n",
      "Epoch 10/500\n",
      "13783/13783 [==============================] - 8s 616us/step - loss: 5.5249 - acc: 0.0556\n",
      "Epoch 11/500\n",
      "13783/13783 [==============================] - 9s 617us/step - loss: 5.4616 - acc: 0.0588\n",
      "Epoch 12/500\n",
      "13783/13783 [==============================] - 9s 617us/step - loss: 5.3931 - acc: 0.0646\n",
      "Epoch 13/500\n",
      "13783/13783 [==============================] - 9s 618us/step - loss: 5.3311 - acc: 0.0640\n",
      "Epoch 14/500\n",
      "13783/13783 [==============================] - 9s 622us/step - loss: 5.2669 - acc: 0.0675\n",
      "Epoch 15/500\n",
      "13783/13783 [==============================] - 9s 618us/step - loss: 5.2093 - acc: 0.0712\n",
      "Epoch 16/500\n",
      "13783/13783 [==============================] - 8s 617us/step - loss: 5.1480 - acc: 0.0746\n",
      "Epoch 17/500\n",
      "13783/13783 [==============================] - 9s 620us/step - loss: 5.0887 - acc: 0.0771\n",
      "Epoch 18/500\n",
      "13783/13783 [==============================] - 9s 617us/step - loss: 5.0317 - acc: 0.0797\n",
      "Epoch 19/500\n",
      "13783/13783 [==============================] - 9s 619us/step - loss: 4.9766 - acc: 0.0813\n",
      "Epoch 20/500\n",
      "13783/13783 [==============================] - 9s 619us/step - loss: 4.9246 - acc: 0.0858\n",
      "Epoch 21/500\n",
      "13783/13783 [==============================] - 9s 618us/step - loss: 4.8738 - acc: 0.0861\n",
      "Epoch 22/500\n",
      "13783/13783 [==============================] - 9s 617us/step - loss: 4.8227 - acc: 0.0908\n",
      "Epoch 23/500\n",
      "13783/13783 [==============================] - 8s 617us/step - loss: 4.7706 - acc: 0.0929\n",
      "Epoch 24/500\n",
      "13783/13783 [==============================] - 9s 617us/step - loss: 4.7178 - acc: 0.0969\n",
      "Epoch 25/500\n",
      "13783/13783 [==============================] - 9s 618us/step - loss: 4.6813 - acc: 0.0982\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "zoo and and and trees and and and and and and and and and and a grinch and and and and and the grinch and and and and the grinch of and and and and the grinch and and and and and and the grinch and and and and the grinch and and and and and and and the grinch and and and and the grinch and and and and and the grinch and and and and and the grinch\n",
      "Epoch 26/500\n",
      "13783/13783 [==============================] - 9s 619us/step - loss: 4.6305 - acc: 0.0997\n",
      "Epoch 27/500\n",
      "13783/13783 [==============================] - 9s 622us/step - loss: 4.5904 - acc: 0.1048\n",
      "Epoch 28/500\n",
      "13783/13783 [==============================] - 9s 626us/step - loss: 4.5432 - acc: 0.1064\n",
      "Epoch 29/500\n",
      "13783/13783 [==============================] - 9s 622us/step - loss: 4.5013 - acc: 0.1072\n",
      "Epoch 30/500\n",
      "13783/13783 [==============================] - 9s 622us/step - loss: 4.4619 - acc: 0.1099\n",
      "Epoch 31/500\n",
      "13783/13783 [==============================] - 9s 685us/step - loss: 4.4119 - acc: 0.1149\n",
      "Epoch 32/500\n",
      "13783/13783 [==============================] - 9s 635us/step - loss: 4.3778 - acc: 0.1172\n",
      "Epoch 33/500\n",
      "13783/13783 [==============================] - 9s 626us/step - loss: 4.3358 - acc: 0.1188\n",
      "Epoch 34/500\n",
      "13783/13783 [==============================] - 9s 621us/step - loss: 4.2991 - acc: 0.1214\n",
      "Epoch 35/500\n",
      "13783/13783 [==============================] - 9s 621us/step - loss: 4.2570 - acc: 0.1228\n",
      "Epoch 36/500\n",
      "13783/13783 [==============================] - 9s 622us/step - loss: 4.2195 - acc: 0.1280\n",
      "Epoch 37/500\n",
      "13783/13783 [==============================] - 9s 622us/step - loss: 4.1791 - acc: 0.1289\n",
      "Epoch 38/500\n",
      "13783/13783 [==============================] - 9s 623us/step - loss: 4.1432 - acc: 0.1329\n",
      "Epoch 39/500\n",
      "13783/13783 [==============================] - 9s 623us/step - loss: 4.1065 - acc: 0.1347\n",
      "Epoch 40/500\n",
      "13783/13783 [==============================] - 9s 624us/step - loss: 4.0753 - acc: 0.1368\n",
      "Epoch 41/500\n",
      "13783/13783 [==============================] - 9s 625us/step - loss: 4.0402 - acc: 0.1379\n",
      "Epoch 42/500\n",
      "13783/13783 [==============================] - 9s 629us/step - loss: 3.9976 - acc: 0.1431\n",
      "Epoch 43/500\n",
      "13783/13783 [==============================] - 9s 630us/step - loss: 3.9632 - acc: 0.1438\n",
      "Epoch 44/500\n",
      "13783/13783 [==============================] - 9s 625us/step - loss: 3.9254 - acc: 0.1484\n",
      "Epoch 45/500\n",
      "13783/13783 [==============================] - 9s 626us/step - loss: 3.8943 - acc: 0.1540\n",
      "Epoch 46/500\n",
      "13783/13783 [==============================] - 9s 627us/step - loss: 3.8651 - acc: 0.1531\n",
      "Epoch 47/500\n",
      "13783/13783 [==============================] - 9s 627us/step - loss: 3.8303 - acc: 0.1587\n",
      "Epoch 48/500\n",
      "13783/13783 [==============================] - 9s 630us/step - loss: 3.7977 - acc: 0.1624\n",
      "Epoch 49/500\n",
      "13783/13783 [==============================] - 9s 628us/step - loss: 3.7604 - acc: 0.1646\n",
      "Epoch 50/500\n",
      "13783/13783 [==============================] - 9s 626us/step - loss: 3.7317 - acc: 0.1651\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "is and i can hold of i cat not know to i did not know to i will not will will be oh oh i like to i do not like to i like to be what hello you like to i will not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not\n",
      "Epoch 51/500\n",
      "13783/13783 [==============================] - 8s 612us/step - loss: 3.6971 - acc: 0.1706\n",
      "Epoch 52/500\n",
      "13783/13783 [==============================] - 10s 694us/step - loss: 3.6736 - acc: 0.1704\n",
      "Epoch 53/500\n",
      "13783/13783 [==============================] - 9s 664us/step - loss: 3.6359 - acc: 0.1759\n",
      "Epoch 54/500\n",
      "13783/13783 [==============================] - 9s 624us/step - loss: 3.6095 - acc: 0.1820\n",
      "Epoch 55/500\n",
      "13783/13783 [==============================] - 9s 626us/step - loss: 3.5754 - acc: 0.1857\n",
      "Epoch 56/500\n",
      "13783/13783 [==============================] - 9s 620us/step - loss: 3.5428 - acc: 0.1870\n",
      "Epoch 57/500\n",
      "13783/13783 [==============================] - 8s 616us/step - loss: 3.5113 - acc: 0.1915\n",
      "Epoch 58/500\n",
      "13783/13783 [==============================] - 8s 616us/step - loss: 3.4906 - acc: 0.1913\n",
      "Epoch 59/500\n",
      "13783/13783 [==============================] - 9s 670us/step - loss: 3.4542 - acc: 0.2002\n",
      "Epoch 60/500\n",
      "13783/13783 [==============================] - 9s 657us/step - loss: 3.4221 - acc: 0.1986\n",
      "Epoch 61/500\n",
      "13783/13783 [==============================] - 9s 671us/step - loss: 3.3991 - acc: 0.2048\n",
      "Epoch 62/500\n",
      "13783/13783 [==============================] - 12s 866us/step - loss: 3.3740 - acc: 0.2072\n",
      "Epoch 63/500\n",
      "13783/13783 [==============================] - 11s 765us/step - loss: 3.3521 - acc: 0.2104\n",
      "Epoch 64/500\n",
      "13783/13783 [==============================] - 10s 732us/step - loss: 3.3227 - acc: 0.2150\n",
      "Epoch 65/500\n",
      "13783/13783 [==============================] - 10s 760us/step - loss: 3.2978 - acc: 0.2179\n",
      "Epoch 66/500\n",
      "13783/13783 [==============================] - 11s 806us/step - loss: 3.2670 - acc: 0.2199\n",
      "Epoch 67/500\n",
      "13783/13783 [==============================] - 11s 776us/step - loss: 3.2508 - acc: 0.2265\n",
      "Epoch 68/500\n",
      "13783/13783 [==============================] - 11s 822us/step - loss: 3.2161 - acc: 0.2289\n",
      "Epoch 69/500\n",
      "13783/13783 [==============================] - 12s 854us/step - loss: 3.1813 - acc: 0.2327\n",
      "Epoch 70/500\n",
      "13783/13783 [==============================] - 11s 809us/step - loss: 3.1642 - acc: 0.2314\n",
      "Epoch 71/500\n",
      "13783/13783 [==============================] - 9s 642us/step - loss: 3.1520 - acc: 0.2325\n",
      "Epoch 72/500\n",
      "13783/13783 [==============================] - 9s 634us/step - loss: 3.1155 - acc: 0.2410\n",
      "Epoch 73/500\n",
      "13783/13783 [==============================] - 9s 682us/step - loss: 3.0917 - acc: 0.2470\n",
      "Epoch 74/500\n",
      "13783/13783 [==============================] - 9s 652us/step - loss: 3.0643 - acc: 0.2544\n",
      "Epoch 75/500\n",
      "13783/13783 [==============================] - 9s 665us/step - loss: 3.0492 - acc: 0.2502\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "and took the chimney belly sneetches sneetches he stars in the plain belly sneetches had all all all at out in the grinch would theyd sing again it hadnt came came sing the grinch came he more or the grinch were down two all but his grinch thought in his grinch got grew youre his grinch down his heart in the whole heart down out the brought did the whos through his heart down his whos down in his whos\n",
      "Epoch 76/500\n",
      "13783/13783 [==============================] - 9s 664us/step - loss: 3.0167 - acc: 0.2610\n",
      "Epoch 77/500\n",
      "13783/13783 [==============================] - 9s 658us/step - loss: 3.0191 - acc: 0.2565\n",
      "Epoch 78/500\n",
      "13783/13783 [==============================] - 9s 666us/step - loss: 2.9864 - acc: 0.2626\n",
      "Epoch 79/500\n",
      "13783/13783 [==============================] - 9s 689us/step - loss: 2.9658 - acc: 0.2645\n",
      "Epoch 80/500\n",
      "13783/13783 [==============================] - 10s 715us/step - loss: 2.9392 - acc: 0.2737\n",
      "Epoch 81/500\n",
      "13783/13783 [==============================] - 9s 689us/step - loss: 2.9163 - acc: 0.2777\n",
      "Epoch 82/500\n",
      "13783/13783 [==============================] - 10s 690us/step - loss: 2.8861 - acc: 0.2771\n",
      "Epoch 83/500\n",
      "13783/13783 [==============================] - 9s 668us/step - loss: 2.8831 - acc: 0.2780\n",
      "Epoch 84/500\n",
      "13783/13783 [==============================] - 9s 675us/step - loss: 2.8580 - acc: 0.2842\n",
      "Epoch 85/500\n",
      "13783/13783 [==============================] - 9s 675us/step - loss: 2.8386 - acc: 0.2808\n",
      "Epoch 86/500\n",
      "13783/13783 [==============================] - 10s 732us/step - loss: 2.8254 - acc: 0.2864\n",
      "Epoch 87/500\n",
      "13783/13783 [==============================] - 10s 718us/step - loss: 2.8019 - acc: 0.2965\n",
      "Epoch 88/500\n",
      "13783/13783 [==============================] - 11s 797us/step - loss: 2.7742 - acc: 0.2970\n",
      "Epoch 89/500\n",
      "13783/13783 [==============================] - 12s 870us/step - loss: 2.7626 - acc: 0.2990\n",
      "Epoch 90/500\n",
      "13783/13783 [==============================] - 12s 879us/step - loss: 2.7495 - acc: 0.3047\n",
      "Epoch 91/500\n",
      "13783/13783 [==============================] - 12s 868us/step - loss: 2.7185 - acc: 0.3081\n",
      "Epoch 92/500\n",
      "13783/13783 [==============================] - 12s 842us/step - loss: 2.6990 - acc: 0.3153\n",
      "Epoch 93/500\n",
      "13783/13783 [==============================] - 12s 837us/step - loss: 2.6972 - acc: 0.3173\n",
      "Epoch 94/500\n",
      "13783/13783 [==============================] - 12s 837us/step - loss: 2.6650 - acc: 0.3176\n",
      "Epoch 95/500\n",
      "13783/13783 [==============================] - 12s 844us/step - loss: 2.6554 - acc: 0.3195\n",
      "Epoch 96/500\n",
      "13783/13783 [==============================] - 12s 853us/step - loss: 2.6313 - acc: 0.3277\n",
      "Epoch 97/500\n",
      "13783/13783 [==============================] - 12s 837us/step - loss: 2.6220 - acc: 0.3284\n",
      "Epoch 98/500\n",
      "13783/13783 [==============================] - 11s 834us/step - loss: 2.6057 - acc: 0.3308\n",
      "Epoch 99/500\n",
      "13783/13783 [==============================] - 12s 834us/step - loss: 2.5729 - acc: 0.3409\n",
      "Epoch 100/500\n",
      "13783/13783 [==============================] - 12s 843us/step - loss: 2.5791 - acc: 0.3407\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "may clean up away sit what you know said the cat but the hat but look at me no what said the cat not like said the cat not no hear it should not you know them the cat i will show them our house said the cat in the hat but the cat came look the cat came look the house in the fish in the hat they good lots of is tricks out of is not not have\n",
      "Epoch 101/500\n",
      "13783/13783 [==============================] - 11s 833us/step - loss: 2.5510 - acc: 0.3417\n",
      "Epoch 102/500\n",
      "13783/13783 [==============================] - 11s 833us/step - loss: 2.5316 - acc: 0.3476\n",
      "Epoch 103/500\n",
      "13783/13783 [==============================] - 11s 833us/step - loss: 2.5284 - acc: 0.3452\n",
      "Epoch 104/500\n",
      "13783/13783 [==============================] - 11s 819us/step - loss: 2.5172 - acc: 0.3462\n",
      "Epoch 105/500\n",
      "13783/13783 [==============================] - 11s 828us/step - loss: 2.4813 - acc: 0.3592\n",
      "Epoch 106/500\n",
      "13783/13783 [==============================] - 11s 803us/step - loss: 2.4804 - acc: 0.3622\n",
      "Epoch 107/500\n",
      "13783/13783 [==============================] - 12s 877us/step - loss: 2.4561 - acc: 0.3599\n",
      "Epoch 108/500\n",
      "13783/13783 [==============================] - 12s 844us/step - loss: 2.4439 - acc: 0.3654\n",
      "Epoch 109/500\n",
      "13783/13783 [==============================] - 9s 688us/step - loss: 2.4387 - acc: 0.3683\n",
      "Epoch 110/500\n",
      "13783/13783 [==============================] - 9s 657us/step - loss: 2.4054 - acc: 0.3697\n",
      "Epoch 111/500\n",
      "13783/13783 [==============================] - 9s 658us/step - loss: 2.3909 - acc: 0.3751\n",
      "Epoch 112/500\n",
      "13783/13783 [==============================] - 9s 661us/step - loss: 2.3778 - acc: 0.3806\n",
      "Epoch 113/500\n",
      "13783/13783 [==============================] - 9s 665us/step - loss: 2.3615 - acc: 0.3821\n",
      "Epoch 114/500\n",
      "13783/13783 [==============================] - 9s 662us/step - loss: 2.3657 - acc: 0.3771\n",
      "Epoch 115/500\n",
      "13783/13783 [==============================] - 9s 657us/step - loss: 2.3378 - acc: 0.3908\n",
      "Epoch 116/500\n",
      "13783/13783 [==============================] - 9s 657us/step - loss: 2.3444 - acc: 0.3898\n",
      "Epoch 117/500\n",
      "13783/13783 [==============================] - 9s 659us/step - loss: 2.3122 - acc: 0.3943\n",
      "Epoch 118/500\n",
      "13783/13783 [==============================] - 9s 663us/step - loss: 2.3091 - acc: 0.3967\n",
      "Epoch 119/500\n",
      "13783/13783 [==============================] - 9s 663us/step - loss: 2.2693 - acc: 0.4083\n",
      "Epoch 120/500\n",
      "13783/13783 [==============================] - 9s 661us/step - loss: 2.2692 - acc: 0.4059\n",
      "Epoch 121/500\n",
      "13783/13783 [==============================] - 9s 662us/step - loss: 2.2587 - acc: 0.3985\n",
      "Epoch 122/500\n",
      "13783/13783 [==============================] - 9s 662us/step - loss: 2.2506 - acc: 0.4029\n",
      "Epoch 123/500\n",
      "13783/13783 [==============================] - 9s 660us/step - loss: 2.2397 - acc: 0.4080\n",
      "Epoch 124/500\n",
      "13783/13783 [==============================] - 9s 665us/step - loss: 2.2325 - acc: 0.4099\n",
      "Epoch 125/500\n",
      "13783/13783 [==============================] - 9s 660us/step - loss: 2.2114 - acc: 0.4121\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "zoo gets beasts people never never ask him your young will capture new will your new beasts and keeper your new theres you paid theres you paid him what is do do the tree im will may smeary at back you a throat and no beasts and a lorax belly guy i wont cant know to is the best away then your plain away day then was none that was her bad her bad her done too too too too\n",
      "Epoch 126/500\n",
      "13783/13783 [==============================] - 9s 665us/step - loss: 2.1822 - acc: 0.4202\n",
      "Epoch 127/500\n",
      "13783/13783 [==============================] - 9s 663us/step - loss: 2.1817 - acc: 0.4191\n",
      "Epoch 128/500\n",
      "13783/13783 [==============================] - 9s 662us/step - loss: 2.1741 - acc: 0.4172\n",
      "Epoch 129/500\n",
      "13783/13783 [==============================] - 9s 662us/step - loss: 2.1565 - acc: 0.4251\n",
      "Epoch 130/500\n",
      "13783/13783 [==============================] - 9s 663us/step - loss: 2.1502 - acc: 0.4304\n",
      "Epoch 131/500\n",
      "13783/13783 [==============================] - 9s 663us/step - loss: 2.1303 - acc: 0.4278\n",
      "Epoch 132/500\n",
      "13783/13783 [==============================] - 9s 662us/step - loss: 2.1171 - acc: 0.4392\n",
      "Epoch 133/500\n",
      "13783/13783 [==============================] - 9s 663us/step - loss: 2.1038 - acc: 0.4381\n",
      "Epoch 134/500\n",
      "13783/13783 [==============================] - 9s 662us/step - loss: 2.0924 - acc: 0.4396\n",
      "Epoch 135/500\n",
      "13783/13783 [==============================] - 9s 663us/step - loss: 2.0968 - acc: 0.4371\n",
      "Epoch 136/500\n",
      "13783/13783 [==============================] - 9s 668us/step - loss: 2.0711 - acc: 0.4499\n",
      "Epoch 137/500\n",
      "13783/13783 [==============================] - 9s 662us/step - loss: 2.0623 - acc: 0.4481\n",
      "Epoch 138/500\n",
      "13783/13783 [==============================] - 9s 663us/step - loss: 2.0573 - acc: 0.4461\n",
      "Epoch 139/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13783/13783 [==============================] - 9s 662us/step - loss: 2.0297 - acc: 0.4543\n",
      "Epoch 140/500\n",
      "13783/13783 [==============================] - 9s 662us/step - loss: 2.0193 - acc: 0.4598\n",
      "Epoch 141/500\n",
      "13783/13783 [==============================] - 9s 657us/step - loss: 2.0214 - acc: 0.4536\n",
      "Epoch 142/500\n",
      "13783/13783 [==============================] - 9s 658us/step - loss: 2.0066 - acc: 0.4572\n",
      "Epoch 143/500\n",
      "13783/13783 [==============================] - 9s 664us/step - loss: 1.9964 - acc: 0.4573\n",
      "Epoch 144/500\n",
      "13783/13783 [==============================] - 9s 662us/step - loss: 1.9947 - acc: 0.4574\n",
      "Epoch 145/500\n",
      "13783/13783 [==============================] - 9s 660us/step - loss: 1.9669 - acc: 0.4650\n",
      "Epoch 146/500\n",
      "13783/13783 [==============================] - 9s 661us/step - loss: 1.9723 - acc: 0.4621\n",
      "Epoch 147/500\n",
      "13783/13783 [==============================] - 9s 658us/step - loss: 1.9505 - acc: 0.4691\n",
      "Epoch 148/500\n",
      "13783/13783 [==============================] - 9s 661us/step - loss: 1.9526 - acc: 0.4684\n",
      "Epoch 149/500\n",
      "13783/13783 [==============================] - 9s 661us/step - loss: 1.9406 - acc: 0.4692\n",
      "Epoch 150/500\n",
      "13783/13783 [==============================] - 9s 662us/step - loss: 1.9254 - acc: 0.4712\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "and go down back and the lazy air and mayzie it opener and down it with snoots in the plain belly sneetches why all something but my egg why sit again it wire but the wire now the plain belly sneetches they roast beast its my grinch were aiming making thneeds making flip flapping north chugs and fast but he sing ready small youre the run of the lorax ler lifted down back the chimney on the plain belly sneetches\n",
      "Epoch 151/500\n",
      "13783/13783 [==============================] - 9s 661us/step - loss: 1.9079 - acc: 0.4822\n",
      "Epoch 152/500\n",
      "13783/13783 [==============================] - 9s 664us/step - loss: 1.9147 - acc: 0.4774\n",
      "Epoch 153/500\n",
      "13783/13783 [==============================] - 9s 665us/step - loss: 1.8914 - acc: 0.4825\n",
      "Epoch 154/500\n",
      "13783/13783 [==============================] - 9s 666us/step - loss: 1.8787 - acc: 0.4875\n",
      "Epoch 155/500\n",
      "13783/13783 [==============================] - 9s 661us/step - loss: 1.8655 - acc: 0.4873\n",
      "Epoch 156/500\n",
      "13783/13783 [==============================] - 9s 660us/step - loss: 1.8645 - acc: 0.4894\n",
      "Epoch 157/500\n",
      "13783/13783 [==============================] - 9s 660us/step - loss: 1.8544 - acc: 0.4917\n",
      "Epoch 158/500\n",
      "13783/13783 [==============================] - 9s 660us/step - loss: 1.8535 - acc: 0.4928\n",
      "Epoch 159/500\n",
      "13783/13783 [==============================] - 9s 663us/step - loss: 1.8421 - acc: 0.4932\n",
      "Epoch 160/500\n",
      "13783/13783 [==============================] - 9s 662us/step - loss: 1.8289 - acc: 0.4954\n",
      "Epoch 161/500\n",
      "13783/13783 [==============================] - 9s 662us/step - loss: 1.8030 - acc: 0.5036\n",
      "Epoch 162/500\n",
      "13783/13783 [==============================] - 9s 661us/step - loss: 1.8292 - acc: 0.4987\n",
      "Epoch 163/500\n",
      "13783/13783 [==============================] - 9s 660us/step - loss: 1.7988 - acc: 0.5050\n",
      "Epoch 164/500\n",
      "13783/13783 [==============================] - 9s 661us/step - loss: 1.7763 - acc: 0.5060\n",
      "Epoch 165/500\n",
      "13783/13783 [==============================] - 9s 666us/step - loss: 1.7881 - acc: 0.5042\n",
      "Epoch 166/500\n",
      "13783/13783 [==============================] - 9s 663us/step - loss: 1.7820 - acc: 0.5077\n",
      "Epoch 167/500\n",
      "13783/13783 [==============================] - 9s 659us/step - loss: 1.7610 - acc: 0.5134\n",
      "Epoch 168/500\n",
      "13783/13783 [==============================] - 9s 660us/step - loss: 1.7378 - acc: 0.5140\n",
      "Epoch 169/500\n",
      "13783/13783 [==============================] - 9s 675us/step - loss: 1.7468 - acc: 0.5108\n",
      "Epoch 170/500\n",
      "13783/13783 [==============================] - 9s 661us/step - loss: 1.7431 - acc: 0.5193\n",
      "Epoch 171/500\n",
      "13783/13783 [==============================] - 9s 663us/step - loss: 1.7367 - acc: 0.5222\n",
      "Epoch 172/500\n",
      "13783/13783 [==============================] - 9s 663us/step - loss: 1.7186 - acc: 0.5209\n",
      "Epoch 173/500\n",
      "13783/13783 [==============================] - 9s 661us/step - loss: 1.7121 - acc: 0.5228\n",
      "Epoch 174/500\n",
      "13783/13783 [==============================] - 9s 663us/step - loss: 1.6922 - acc: 0.5294\n",
      "Epoch 175/500\n",
      "13783/13783 [==============================] - 9s 668us/step - loss: 1.7050 - acc: 0.5251\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "stop search of the mind you come youll stop nervously boys from the fizza ma wizza ma dill with the worlds tufts and he pulls in the lerkim where he sweet dreams in the last he who humming theyre was if but whoville he came it and his heart was any stood youre under the roof he perfectly feel frown after youre without gummed my puzzler he pulls of his lerkim where their gills from the cave of a cave\n",
      "Epoch 176/500\n",
      "13783/13783 [==============================] - 9s 661us/step - loss: 1.6900 - acc: 0.5313\n",
      "Epoch 177/500\n",
      "13783/13783 [==============================] - 9s 660us/step - loss: 1.6900 - acc: 0.5275\n",
      "Epoch 178/500\n",
      "13783/13783 [==============================] - 9s 663us/step - loss: 1.6744 - acc: 0.5291\n",
      "Epoch 179/500\n",
      "13783/13783 [==============================] - 9s 667us/step - loss: 1.6573 - acc: 0.5360\n",
      "Epoch 180/500\n",
      "13783/13783 [==============================] - 9s 661us/step - loss: 1.6530 - acc: 0.5404\n",
      "Epoch 181/500\n",
      "13783/13783 [==============================] - 9s 661us/step - loss: 1.6597 - acc: 0.5309\n",
      "Epoch 182/500\n",
      "13783/13783 [==============================] - 9s 662us/step - loss: 1.6339 - acc: 0.5375\n",
      "Epoch 183/500\n",
      "13783/13783 [==============================] - 9s 659us/step - loss: 1.6323 - acc: 0.5415\n",
      "Epoch 184/500\n",
      "13783/13783 [==============================] - 9s 660us/step - loss: 1.6271 - acc: 0.5434\n",
      "Epoch 185/500\n",
      "13783/13783 [==============================] - 9s 659us/step - loss: 1.6201 - acc: 0.5459\n",
      "Epoch 186/500\n",
      "13783/13783 [==============================] - 9s 661us/step - loss: 1.6046 - acc: 0.5544\n",
      "Epoch 187/500\n",
      "13783/13783 [==============================] - 9s 662us/step - loss: 1.6120 - acc: 0.5499\n",
      "Epoch 188/500\n",
      "13783/13783 [==============================] - 9s 660us/step - loss: 1.5787 - acc: 0.5588\n",
      "Epoch 189/500\n",
      "13783/13783 [==============================] - 9s 661us/step - loss: 1.5925 - acc: 0.5522\n",
      "Epoch 190/500\n",
      "13783/13783 [==============================] - 9s 659us/step - loss: 1.5836 - acc: 0.5572\n",
      "Epoch 191/500\n",
      "13783/13783 [==============================] - 9s 662us/step - loss: 1.5697 - acc: 0.5563\n",
      "Epoch 192/500\n",
      "13783/13783 [==============================] - 9s 666us/step - loss: 1.5579 - acc: 0.5603\n",
      "Epoch 193/500\n",
      "13783/13783 [==============================] - 9s 658us/step - loss: 1.5517 - acc: 0.5661\n",
      "Epoch 194/500\n",
      "13783/13783 [==============================] - 9s 660us/step - loss: 1.5492 - acc: 0.5669\n",
      "Epoch 195/500\n",
      "13783/13783 [==============================] - 9s 659us/step - loss: 1.5451 - acc: 0.5647\n",
      "Epoch 196/500\n",
      "13783/13783 [==============================] - 9s 662us/step - loss: 1.5453 - acc: 0.5639\n",
      "Epoch 197/500\n",
      "13783/13783 [==============================] - 9s 663us/step - loss: 1.5354 - acc: 0.5669\n",
      "Epoch 198/500\n",
      "13783/13783 [==============================] - 9s 663us/step - loss: 1.5125 - acc: 0.5762\n",
      "Epoch 199/500\n",
      "13783/13783 [==============================] - 9s 661us/step - loss: 1.4970 - acc: 0.5785\n",
      "Epoch 200/500\n",
      "13783/13783 [==============================] - 9s 663us/step - loss: 1.5103 - acc: 0.5736\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "ive kinks up on sitting here everyone here in the poor sneeze im play step hang open or or a shoes or theyre for night my cave and theyre or ones is ones him out a make its another chance well games is im find too him him games is come but you im well youll too dont too too too youll just for for no say your new trees fleas broom rose rose fleas books to choose to get\n",
      "Epoch 201/500\n",
      "13783/13783 [==============================] - 9s 661us/step - loss: 1.5100 - acc: 0.5740\n",
      "Epoch 202/500\n",
      "13783/13783 [==============================] - 9s 664us/step - loss: 1.5107 - acc: 0.5695\n",
      "Epoch 203/500\n",
      "13783/13783 [==============================] - 9s 663us/step - loss: 1.4943 - acc: 0.5787\n",
      "Epoch 204/500\n",
      "13783/13783 [==============================] - 9s 662us/step - loss: 1.4896 - acc: 0.5743\n",
      "Epoch 205/500\n",
      "13783/13783 [==============================] - 9s 665us/step - loss: 1.4870 - acc: 0.5749\n",
      "Epoch 206/500\n",
      "13783/13783 [==============================] - 9s 662us/step - loss: 1.4773 - acc: 0.5826\n",
      "Epoch 207/500\n",
      "13783/13783 [==============================] - 9s 661us/step - loss: 1.4677 - acc: 0.5831\n",
      "Epoch 208/500\n",
      "13783/13783 [==============================] - 9s 657us/step - loss: 1.4769 - acc: 0.5823\n",
      "Epoch 209/500\n",
      "13783/13783 [==============================] - 9s 635us/step - loss: 1.4406 - acc: 0.5877\n",
      "Epoch 210/500\n",
      "13783/13783 [==============================] - 9s 637us/step - loss: 1.4621 - acc: 0.5818\n",
      "Epoch 211/500\n",
      "13783/13783 [==============================] - 9s 641us/step - loss: 1.4422 - acc: 0.5939\n",
      "Epoch 212/500\n",
      "13783/13783 [==============================] - 9s 638us/step - loss: 1.4383 - acc: 0.5888\n",
      "Epoch 213/500\n",
      "13783/13783 [==============================] - 9s 638us/step - loss: 1.4297 - acc: 0.5914\n",
      "Epoch 214/500\n",
      "13783/13783 [==============================] - 10s 694us/step - loss: 1.4307 - acc: 0.5859\n",
      "Epoch 215/500\n",
      "13783/13783 [==============================] - 9s 663us/step - loss: 1.4455 - acc: 0.5890\n",
      "Epoch 216/500\n",
      "13783/13783 [==============================] - 9s 638us/step - loss: 1.4054 - acc: 0.5987\n",
      "Epoch 217/500\n",
      "13783/13783 [==============================] - 9s 640us/step - loss: 1.4004 - acc: 0.6026\n",
      "Epoch 218/500\n",
      "13783/13783 [==============================] - 9s 643us/step - loss: 1.3906 - acc: 0.5997\n",
      "Epoch 219/500\n",
      "13783/13783 [==============================] - 9s 643us/step - loss: 1.4003 - acc: 0.5991\n",
      "Epoch 220/500\n",
      "13783/13783 [==============================] - 9s 639us/step - loss: 1.3803 - acc: 0.6072\n",
      "Epoch 221/500\n",
      "13783/13783 [==============================] - 9s 638us/step - loss: 1.3897 - acc: 0.6087\n",
      "Epoch 222/500\n",
      "13783/13783 [==============================] - 9s 640us/step - loss: 1.3763 - acc: 0.6063\n",
      "Epoch 223/500\n",
      "13783/13783 [==============================] - 9s 637us/step - loss: 1.3780 - acc: 0.6021\n",
      "Epoch 224/500\n",
      "13783/13783 [==============================] - 9s 638us/step - loss: 1.3668 - acc: 0.6081\n",
      "Epoch 225/500\n",
      "13783/13783 [==============================] - 9s 642us/step - loss: 1.3750 - acc: 0.6061\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "way off in the palm beach and having such worried mad their before had again in their be fixing back off out out again again their machines up back from again a well it care out but his lurch with knew all the best left the toys and he food and the plain he left on he big were them but fifty ball not like to you need how the egg who can people laughed i laughed on the star\n",
      "Epoch 226/500\n",
      "13783/13783 [==============================] - 9s 638us/step - loss: 1.3725 - acc: 0.6064\n",
      "Epoch 227/500\n",
      "13783/13783 [==============================] - 9s 639us/step - loss: 1.3552 - acc: 0.6124\n",
      "Epoch 228/500\n",
      "13783/13783 [==============================] - 9s 642us/step - loss: 1.3362 - acc: 0.6139\n",
      "Epoch 229/500\n",
      "13783/13783 [==============================] - 9s 640us/step - loss: 1.3463 - acc: 0.6134\n",
      "Epoch 230/500\n",
      "13783/13783 [==============================] - 9s 644us/step - loss: 1.3227 - acc: 0.6213\n",
      "Epoch 231/500\n",
      "13783/13783 [==============================] - 9s 640us/step - loss: 1.3295 - acc: 0.6171\n",
      "Epoch 232/500\n",
      "13783/13783 [==============================] - 9s 647us/step - loss: 1.3298 - acc: 0.6229\n",
      "Epoch 233/500\n",
      "13783/13783 [==============================] - 9s 641us/step - loss: 1.3091 - acc: 0.6236\n",
      "Epoch 234/500\n",
      "13783/13783 [==============================] - 9s 639us/step - loss: 1.3086 - acc: 0.6245\n",
      "Epoch 235/500\n",
      "13783/13783 [==============================] - 9s 640us/step - loss: 1.3113 - acc: 0.6248\n",
      "Epoch 236/500\n",
      "13783/13783 [==============================] - 9s 648us/step - loss: 1.2789 - acc: 0.6270\n",
      "Epoch 237/500\n",
      "13783/13783 [==============================] - 9s 644us/step - loss: 1.2998 - acc: 0.6261\n",
      "Epoch 238/500\n",
      "13783/13783 [==============================] - 9s 643us/step - loss: 1.2984 - acc: 0.6302\n",
      "Epoch 239/500\n",
      "13783/13783 [==============================] - 9s 640us/step - loss: 1.2911 - acc: 0.6272\n",
      "Epoch 240/500\n",
      "13783/13783 [==============================] - 9s 642us/step - loss: 1.2794 - acc: 0.6316\n",
      "Epoch 241/500\n",
      "13783/13783 [==============================] - 9s 640us/step - loss: 1.2792 - acc: 0.6299\n",
      "Epoch 242/500\n",
      "13783/13783 [==============================] - 9s 644us/step - loss: 1.2648 - acc: 0.6316\n",
      "Epoch 243/500\n",
      "13783/13783 [==============================] - 9s 640us/step - loss: 1.2704 - acc: 0.6307\n",
      "Epoch 244/500\n",
      "13783/13783 [==============================] - 9s 642us/step - loss: 1.2533 - acc: 0.6361\n",
      "Epoch 245/500\n",
      "13783/13783 [==============================] - 9s 643us/step - loss: 1.2413 - acc: 0.6373\n",
      "Epoch 246/500\n",
      "13783/13783 [==============================] - 9s 645us/step - loss: 1.2591 - acc: 0.6366\n",
      "Epoch 247/500\n",
      "13783/13783 [==============================] - 9s 640us/step - loss: 1.2551 - acc: 0.6384\n",
      "Epoch 248/500\n",
      "13783/13783 [==============================] - 9s 640us/step - loss: 1.2411 - acc: 0.6395\n",
      "Epoch 249/500\n",
      "13783/13783 [==============================] - 9s 642us/step - loss: 1.2446 - acc: 0.6401\n",
      "Epoch 250/500\n",
      "13783/13783 [==============================] - 9s 640us/step - loss: 1.2269 - acc: 0.6448\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "cat ran now i know what now dont hard to away no ran he ran him he picked to a tip and the milk i can hold to game from our box on my net and my fan on the milk on the milk and the milk and the head and look and that kites and the hall ran out then he went one step where he head in the milk in the t he night in horton ran her\n",
      "Epoch 251/500\n",
      "13783/13783 [==============================] - 9s 639us/step - loss: 1.2433 - acc: 0.6417\n",
      "Epoch 252/500\n",
      "13783/13783 [==============================] - 9s 646us/step - loss: 1.2279 - acc: 0.6449\n",
      "Epoch 253/500\n",
      "13783/13783 [==============================] - 9s 639us/step - loss: 1.2237 - acc: 0.6432\n",
      "Epoch 254/500\n",
      "13783/13783 [==============================] - 9s 638us/step - loss: 1.2095 - acc: 0.6457\n",
      "Epoch 255/500\n",
      "13783/13783 [==============================] - 9s 643us/step - loss: 1.2192 - acc: 0.6494\n",
      "Epoch 256/500\n",
      "13783/13783 [==============================] - 9s 639us/step - loss: 1.2102 - acc: 0.6516\n",
      "Epoch 257/500\n",
      "13783/13783 [==============================] - 9s 642us/step - loss: 1.1996 - acc: 0.6539\n",
      "Epoch 258/500\n",
      "13783/13783 [==============================] - 9s 639us/step - loss: 1.1862 - acc: 0.6533\n",
      "Epoch 259/500\n",
      "13783/13783 [==============================] - 9s 646us/step - loss: 1.1952 - acc: 0.6531\n",
      "Epoch 260/500\n",
      "13783/13783 [==============================] - 9s 639us/step - loss: 1.1907 - acc: 0.6525\n",
      "Epoch 261/500\n",
      "13783/13783 [==============================] - 9s 639us/step - loss: 1.1715 - acc: 0.6568\n",
      "Epoch 262/500\n",
      "13783/13783 [==============================] - 9s 641us/step - loss: 1.1779 - acc: 0.6551\n",
      "Epoch 263/500\n",
      "13783/13783 [==============================] - 9s 639us/step - loss: 1.1690 - acc: 0.6585\n",
      "Epoch 264/500\n",
      "13783/13783 [==============================] - 9s 642us/step - loss: 1.1695 - acc: 0.6592\n",
      "Epoch 265/500\n",
      "13783/13783 [==============================] - 9s 640us/step - loss: 1.1589 - acc: 0.6611\n",
      "Epoch 266/500\n",
      "13783/13783 [==============================] - 9s 642us/step - loss: 1.1438 - acc: 0.6658\n",
      "Epoch 267/500\n",
      "13783/13783 [==============================] - 9s 639us/step - loss: 1.1557 - acc: 0.6615\n",
      "Epoch 268/500\n",
      "13783/13783 [==============================] - 9s 639us/step - loss: 1.1519 - acc: 0.6654\n",
      "Epoch 269/500\n",
      "13783/13783 [==============================] - 9s 640us/step - loss: 1.1457 - acc: 0.6653\n",
      "Epoch 270/500\n",
      "13783/13783 [==============================] - 9s 642us/step - loss: 1.1521 - acc: 0.6590\n",
      "Epoch 271/500\n",
      "13783/13783 [==============================] - 9s 641us/step - loss: 1.1493 - acc: 0.6676\n",
      "Epoch 272/500\n",
      "13783/13783 [==============================] - 9s 644us/step - loss: 1.1384 - acc: 0.6671\n",
      "Epoch 273/500\n",
      "13783/13783 [==============================] - 9s 643us/step - loss: 1.1243 - acc: 0.6751\n",
      "Epoch 274/500\n",
      "13783/13783 [==============================] - 9s 642us/step - loss: 1.1214 - acc: 0.6723\n",
      "Epoch 275/500\n",
      "13783/13783 [==============================] - 9s 639us/step - loss: 1.1152 - acc: 0.6785\n",
      "Checkpointing the model...\n",
      "Generating Text...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks bricks and blocks on knox on box now now now chicks and clocks knox bricks and blocks sir lets do tricks tricks in the minute to ticks and clocks sir first do the most trick its a mind sir you call it some yes hes youll lot and no idea whether the tree that you will find some beasts and i let them in near you meant it all one hundred per cent one uncles and i have me\n",
      "Epoch 276/500\n",
      "13783/13783 [==============================] - 9s 641us/step - loss: 1.1175 - acc: 0.6735\n",
      "Epoch 277/500\n",
      "13783/13783 [==============================] - 9s 640us/step - loss: 1.1172 - acc: 0.6734\n",
      "Epoch 278/500\n",
      "13783/13783 [==============================] - 9s 639us/step - loss: 1.0995 - acc: 0.6758\n",
      "Epoch 279/500\n",
      "13783/13783 [==============================] - 9s 641us/step - loss: 1.1201 - acc: 0.6705\n",
      "Epoch 280/500\n",
      "13783/13783 [==============================] - 9s 637us/step - loss: 1.0974 - acc: 0.6778\n",
      "Epoch 281/500\n",
      "13783/13783 [==============================] - 9s 639us/step - loss: 1.0928 - acc: 0.6824\n",
      "Epoch 282/500\n",
      "13783/13783 [==============================] - 9s 641us/step - loss: 1.0833 - acc: 0.6843\n",
      "Epoch 283/500\n",
      "13783/13783 [==============================] - 9s 638us/step - loss: 1.0982 - acc: 0.6823\n",
      "Epoch 284/500\n",
      "13783/13783 [==============================] - 9s 638us/step - loss: 1.1001 - acc: 0.6843\n",
      "Epoch 285/500\n",
      "13783/13783 [==============================] - 9s 637us/step - loss: 1.0818 - acc: 0.6836\n",
      "Epoch 286/500\n",
      "13783/13783 [==============================] - 9s 644us/step - loss: 1.0915 - acc: 0.6800\n",
      "Epoch 287/500\n",
      "13783/13783 [==============================] - 9s 641us/step - loss: 1.0926 - acc: 0.6792\n",
      "Epoch 288/500\n",
      "13783/13783 [==============================] - 9s 639us/step - loss: 1.0709 - acc: 0.6863\n",
      "Epoch 289/500\n",
      "13783/13783 [==============================] - 9s 637us/step - loss: 1.0728 - acc: 0.6858\n",
      "Epoch 290/500\n",
      "13783/13783 [==============================] - 8s 611us/step - loss: 1.0617 - acc: 0.6927\n",
      "Epoch 291/500\n",
      "13783/13783 [==============================] - 8s 611us/step - loss: 1.0642 - acc: 0.6893\n",
      "Epoch 292/500\n",
      "13783/13783 [==============================] - 8s 612us/step - loss: 1.0563 - acc: 0.6998\n",
      "Epoch 293/500\n",
      "13783/13783 [==============================] - 8s 616us/step - loss: 1.0484 - acc: 0.6935\n",
      "Epoch 294/500\n",
      "13783/13783 [==============================] - 8s 610us/step - loss: 1.0691 - acc: 0.6912\n",
      "Epoch 295/500\n",
      "13783/13783 [==============================] - 8s 611us/step - loss: 1.0505 - acc: 0.6923\n",
      "Epoch 296/500\n",
      "13783/13783 [==============================] - 8s 614us/step - loss: 1.0421 - acc: 0.6973\n",
      "Epoch 297/500\n",
      "13783/13783 [==============================] - 8s 609us/step - loss: 1.0388 - acc: 0.6940\n",
      "Epoch 298/500\n",
      "13783/13783 [==============================] - 8s 610us/step - loss: 1.0496 - acc: 0.6945\n",
      "Epoch 299/500\n",
      "13783/13783 [==============================] - 8s 613us/step - loss: 1.0418 - acc: 0.6916\n",
      "Epoch 300/500\n",
      "13783/13783 [==============================] - 9s 617us/step - loss: 1.0492 - acc: 0.6911\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "for the trees have no tongues and im asking you sir at a top if my lungs he was upset as went with he grim hung by the egg where the men had around at he had but my small all without brand nothing to christmas other hole in the snapped or the star belly left in whoville tell the more who the best lers which his whole once ler unless making someone silk but under the far the trees\n",
      "Epoch 301/500\n",
      "13783/13783 [==============================] - 8s 612us/step - loss: 1.0360 - acc: 0.6939\n",
      "Epoch 302/500\n",
      "13783/13783 [==============================] - 8s 611us/step - loss: 1.0324 - acc: 0.6952\n",
      "Epoch 303/500\n",
      "13783/13783 [==============================] - 8s 614us/step - loss: 1.0222 - acc: 0.6948\n",
      "Epoch 304/500\n",
      "13783/13783 [==============================] - 9s 624us/step - loss: 1.0232 - acc: 0.6984\n",
      "Epoch 305/500\n",
      "13783/13783 [==============================] - 8s 611us/step - loss: 1.0054 - acc: 0.7050\n",
      "Epoch 306/500\n",
      "13783/13783 [==============================] - 8s 615us/step - loss: 1.0152 - acc: 0.7037\n",
      "Epoch 307/500\n",
      "13783/13783 [==============================] - 8s 614us/step - loss: 1.0050 - acc: 0.7036\n",
      "Epoch 308/500\n",
      "13783/13783 [==============================] - 8s 610us/step - loss: 1.0162 - acc: 0.7027\n",
      "Epoch 309/500\n",
      "13783/13783 [==============================] - 8s 611us/step - loss: 1.0149 - acc: 0.7002\n",
      "Epoch 310/500\n",
      "13783/13783 [==============================] - 8s 612us/step - loss: 1.0091 - acc: 0.7038\n",
      "Epoch 311/500\n",
      "13783/13783 [==============================] - 8s 614us/step - loss: 0.9904 - acc: 0.7078\n",
      "Epoch 312/500\n",
      "13783/13783 [==============================] - 8s 599us/step - loss: 0.9967 - acc: 0.7070\n",
      "Epoch 313/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 1.0005 - acc: 0.7064\n",
      "Epoch 314/500\n",
      "13783/13783 [==============================] - 8s 598us/step - loss: 0.9978 - acc: 0.7022\n",
      "Epoch 315/500\n",
      "13783/13783 [==============================] - 8s 601us/step - loss: 0.9890 - acc: 0.7078\n",
      "Epoch 316/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.9875 - acc: 0.7141\n",
      "Epoch 317/500\n",
      "13783/13783 [==============================] - 8s 594us/step - loss: 0.9833 - acc: 0.7055\n",
      "Epoch 318/500\n",
      "13783/13783 [==============================] - 8s 598us/step - loss: 0.9750 - acc: 0.7139\n",
      "Epoch 319/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.9735 - acc: 0.7165\n",
      "Epoch 320/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.9761 - acc: 0.7097\n",
      "Epoch 321/500\n",
      "13783/13783 [==============================] - 8s 600us/step - loss: 0.9662 - acc: 0.7210\n",
      "Epoch 322/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.9539 - acc: 0.7216\n",
      "Epoch 323/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.9590 - acc: 0.7127\n",
      "Epoch 324/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.9595 - acc: 0.7186\n",
      "Epoch 325/500\n",
      "13783/13783 [==============================] - 8s 598us/step - loss: 0.9479 - acc: 0.7187\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "could you in a car eat them eat them here they are i would not could not in a car you may eat them here a car i would eat you could not in a car you let me may you would not in a tree i would not could not in a tree not in a house not with a mouse i could not eat them in a mouse i do not like them here or there i do\n",
      "Epoch 326/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.9640 - acc: 0.7140\n",
      "Epoch 327/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.9318 - acc: 0.7244\n",
      "Epoch 328/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.9362 - acc: 0.7268\n",
      "Epoch 329/500\n",
      "13783/13783 [==============================] - 8s 604us/step - loss: 0.9332 - acc: 0.7261\n",
      "Epoch 330/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.9368 - acc: 0.7209\n",
      "Epoch 331/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.9417 - acc: 0.7201\n",
      "Epoch 332/500\n",
      "13783/13783 [==============================] - 8s 598us/step - loss: 0.9322 - acc: 0.7243\n",
      "Epoch 333/500\n",
      "13783/13783 [==============================] - 8s 594us/step - loss: 0.9273 - acc: 0.7232\n",
      "Epoch 334/500\n",
      "13783/13783 [==============================] - 8s 598us/step - loss: 0.9188 - acc: 0.7268\n",
      "Epoch 335/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.9194 - acc: 0.7312\n",
      "Epoch 336/500\n",
      "13783/13783 [==============================] - 8s 601us/step - loss: 0.9275 - acc: 0.7267\n",
      "Epoch 337/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.9368 - acc: 0.7288\n",
      "Epoch 338/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.9355 - acc: 0.7257\n",
      "Epoch 339/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.9232 - acc: 0.7284\n",
      "Epoch 340/500\n",
      "13783/13783 [==============================] - 8s 602us/step - loss: 0.8931 - acc: 0.7383\n",
      "Epoch 341/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.9322 - acc: 0.7243\n",
      "Epoch 342/500\n",
      "13783/13783 [==============================] - 8s 599us/step - loss: 0.9183 - acc: 0.7278\n",
      "Epoch 343/500\n",
      "13783/13783 [==============================] - 8s 600us/step - loss: 0.9248 - acc: 0.7281\n",
      "Epoch 344/500\n",
      "13783/13783 [==============================] - 8s 600us/step - loss: 0.8953 - acc: 0.7366\n",
      "Epoch 345/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.9008 - acc: 0.7337\n",
      "Epoch 346/500\n",
      "13783/13783 [==============================] - 8s 594us/step - loss: 0.8919 - acc: 0.7327\n",
      "Epoch 347/500\n",
      "13783/13783 [==============================] - 8s 601us/step - loss: 0.9133 - acc: 0.7279\n",
      "Epoch 348/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.8902 - acc: 0.7318\n",
      "Epoch 349/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.8916 - acc: 0.7353\n",
      "Epoch 350/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.8819 - acc: 0.7342\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "he was here play tricks laughed the cat oh my my no no no i just want to go out of the head keep the hat is his cold is away and a work and so if the hat what no dad is is is we like this our cat out and my cats g cans and and i right up but to you dont think have if you never need more tricks i bet with tricks on i must\n",
      "Epoch 351/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.8898 - acc: 0.7369\n",
      "Epoch 352/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.8826 - acc: 0.7393\n",
      "Epoch 353/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.8846 - acc: 0.7361\n",
      "Epoch 354/500\n",
      "13783/13783 [==============================] - 8s 603us/step - loss: 0.8862 - acc: 0.7366\n",
      "Epoch 355/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.8573 - acc: 0.7432\n",
      "Epoch 356/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.8718 - acc: 0.7398\n",
      "Epoch 357/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.8757 - acc: 0.7410\n",
      "Epoch 358/500\n",
      "13783/13783 [==============================] - 8s 601us/step - loss: 0.8720 - acc: 0.7389\n",
      "Epoch 359/500\n",
      "13783/13783 [==============================] - 8s 598us/step - loss: 0.8658 - acc: 0.7432\n",
      "Epoch 360/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.8415 - acc: 0.7500\n",
      "Epoch 361/500\n",
      "13783/13783 [==============================] - 8s 598us/step - loss: 0.8544 - acc: 0.7477\n",
      "Epoch 362/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.8448 - acc: 0.7487\n",
      "Epoch 363/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.8721 - acc: 0.7401\n",
      "Epoch 364/500\n",
      "13783/13783 [==============================] - 8s 598us/step - loss: 0.8570 - acc: 0.7458\n",
      "Epoch 365/500\n",
      "13783/13783 [==============================] - 8s 600us/step - loss: 0.8466 - acc: 0.7511\n",
      "Epoch 366/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.8407 - acc: 0.7490\n",
      "Epoch 367/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.8589 - acc: 0.7429\n",
      "Epoch 368/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.8387 - acc: 0.7496\n",
      "Epoch 369/500\n",
      "13783/13783 [==============================] - 8s 598us/step - loss: 0.8565 - acc: 0.7473\n",
      "Epoch 370/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.8632 - acc: 0.7425\n",
      "Epoch 371/500\n",
      "13783/13783 [==============================] - 8s 599us/step - loss: 0.8530 - acc: 0.7458\n",
      "Epoch 372/500\n",
      "13783/13783 [==============================] - 8s 599us/step - loss: 0.8343 - acc: 0.7538\n",
      "Epoch 373/500\n",
      "13783/13783 [==============================] - 8s 601us/step - loss: 0.8438 - acc: 0.7477\n",
      "Epoch 374/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.8425 - acc: 0.7511\n",
      "Epoch 375/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.8244 - acc: 0.7563\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "matter what happens this egg must be tended but poor hortons troubles were far far from ended for while horton sat there so faithful so kind three hunters whatever a hunters sounding how how no no catch hes right unless youre alone if the lorax and are youre just whatever youll your dollars bird bird since an egg just each off sitting my buildings get too youve answered answered mayzie you know so was after into back there then he\n",
      "Epoch 376/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.8217 - acc: 0.7588\n",
      "Epoch 377/500\n",
      "13783/13783 [==============================] - 8s 601us/step - loss: 0.8274 - acc: 0.7592\n",
      "Epoch 378/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.8153 - acc: 0.7580\n",
      "Epoch 379/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.8296 - acc: 0.7551\n",
      "Epoch 380/500\n",
      "13783/13783 [==============================] - 8s 598us/step - loss: 0.8327 - acc: 0.7546\n",
      "Epoch 381/500\n",
      "13783/13783 [==============================] - 8s 599us/step - loss: 0.8211 - acc: 0.7559\n",
      "Epoch 382/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.8277 - acc: 0.7548\n",
      "Epoch 383/500\n",
      "13783/13783 [==============================] - 8s 602us/step - loss: 0.8292 - acc: 0.7542\n",
      "Epoch 384/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.8282 - acc: 0.7547\n",
      "Epoch 385/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.8025 - acc: 0.7598\n",
      "Epoch 386/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.8043 - acc: 0.7608\n",
      "Epoch 387/500\n",
      "13783/13783 [==============================] - 8s 603us/step - loss: 0.8199 - acc: 0.7596\n",
      "Epoch 388/500\n",
      "13783/13783 [==============================] - 8s 600us/step - loss: 0.8127 - acc: 0.7566\n",
      "Epoch 389/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.7955 - acc: 0.7591\n",
      "Epoch 390/500\n",
      "13783/13783 [==============================] - 8s 600us/step - loss: 0.8008 - acc: 0.7620\n",
      "Epoch 391/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.7867 - acc: 0.7643\n",
      "Epoch 392/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.7905 - acc: 0.7641\n",
      "Epoch 393/500\n",
      "13783/13783 [==============================] - 8s 598us/step - loss: 0.7963 - acc: 0.7623\n",
      "Epoch 394/500\n",
      "13783/13783 [==============================] - 8s 600us/step - loss: 0.7900 - acc: 0.7616\n",
      "Epoch 395/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.7933 - acc: 0.7620\n",
      "Epoch 396/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.7983 - acc: 0.7628\n",
      "Epoch 397/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.7805 - acc: 0.7667\n",
      "Epoch 398/500\n",
      "13783/13783 [==============================] - 8s 600us/step - loss: 0.7716 - acc: 0.7736\n",
      "Epoch 399/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.7838 - acc: 0.7658\n",
      "Epoch 400/500\n",
      "13783/13783 [==============================] - 8s 598us/step - loss: 0.7799 - acc: 0.7681\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "were still clean and the song of the swomee swans rang out in space one morning i came to this glorious place and he thats the trees the truffula trees the moment or all all my cooks in my snapped how a few elephant awful quite so here it other stop place for the bright clear ma feel finding been he pulls up the lerkim it theyre for tomorrow my mind its his mind just a lead its amazing that\n",
      "Epoch 401/500\n",
      "13783/13783 [==============================] - 8s 600us/step - loss: 0.8088 - acc: 0.7598\n",
      "Epoch 402/500\n",
      "13783/13783 [==============================] - 8s 601us/step - loss: 0.7797 - acc: 0.7656\n",
      "Epoch 403/500\n",
      "13783/13783 [==============================] - 8s 601us/step - loss: 0.7779 - acc: 0.7694\n",
      "Epoch 404/500\n",
      "13783/13783 [==============================] - 8s 598us/step - loss: 0.7752 - acc: 0.7675\n",
      "Epoch 405/500\n",
      "13783/13783 [==============================] - 8s 600us/step - loss: 0.7725 - acc: 0.7695\n",
      "Epoch 406/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.7421 - acc: 0.7792\n",
      "Epoch 407/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.7754 - acc: 0.7713\n",
      "Epoch 408/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.7826 - acc: 0.7701\n",
      "Epoch 409/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13783/13783 [==============================] - 8s 600us/step - loss: 0.7759 - acc: 0.7729\n",
      "Epoch 410/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.7510 - acc: 0.7746\n",
      "Epoch 411/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.7360 - acc: 0.7813\n",
      "Epoch 412/500\n",
      "13783/13783 [==============================] - 8s 598us/step - loss: 0.7548 - acc: 0.7759\n",
      "Epoch 413/500\n",
      "13783/13783 [==============================] - 8s 601us/step - loss: 0.7555 - acc: 0.7794\n",
      "Epoch 414/500\n",
      "13783/13783 [==============================] - 8s 598us/step - loss: 0.7623 - acc: 0.7742\n",
      "Epoch 415/500\n",
      "13783/13783 [==============================] - 8s 598us/step - loss: 0.7588 - acc: 0.7728\n",
      "Epoch 416/500\n",
      "13783/13783 [==============================] - 8s 600us/step - loss: 0.7581 - acc: 0.7754\n",
      "Epoch 417/500\n",
      "13783/13783 [==============================] - 8s 598us/step - loss: 0.7533 - acc: 0.7741\n",
      "Epoch 418/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.7259 - acc: 0.7843\n",
      "Epoch 419/500\n",
      "13783/13783 [==============================] - 8s 599us/step - loss: 0.7402 - acc: 0.7787\n",
      "Epoch 420/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.7320 - acc: 0.7810\n",
      "Epoch 421/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.7279 - acc: 0.7851\n",
      "Epoch 422/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.7313 - acc: 0.7755\n",
      "Epoch 423/500\n",
      "13783/13783 [==============================] - 8s 599us/step - loss: 0.7452 - acc: 0.7743\n",
      "Epoch 424/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.7390 - acc: 0.7785\n",
      "Epoch 425/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.7324 - acc: 0.7823\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "balls and little pink pills oh the things that they did them they did and like it make that one cat but now again at away you have gone out of his head now all pop voom i saw have me is me he should have this fear cat me about said the fish in the hat but all right hands in a box i will show to fall you will be just two thing then the big bed with\n",
      "Epoch 426/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.7350 - acc: 0.7802\n",
      "Epoch 427/500\n",
      "13783/13783 [==============================] - 8s 599us/step - loss: 0.7266 - acc: 0.7791\n",
      "Epoch 428/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.7357 - acc: 0.7799\n",
      "Epoch 429/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.7321 - acc: 0.7818\n",
      "Epoch 430/500\n",
      "13783/13783 [==============================] - 8s 599us/step - loss: 0.7351 - acc: 0.7847\n",
      "Epoch 431/500\n",
      "13783/13783 [==============================] - 8s 601us/step - loss: 0.7291 - acc: 0.7805\n",
      "Epoch 432/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.7118 - acc: 0.7876\n",
      "Epoch 433/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.7321 - acc: 0.7835\n",
      "Epoch 434/500\n",
      "13783/13783 [==============================] - 8s 598us/step - loss: 0.7248 - acc: 0.7815\n",
      "Epoch 435/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.7322 - acc: 0.7819\n",
      "Epoch 436/500\n",
      "13783/13783 [==============================] - 8s 598us/step - loss: 0.6936 - acc: 0.7908\n",
      "Epoch 437/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.7107 - acc: 0.7891\n",
      "Epoch 438/500\n",
      "13783/13783 [==============================] - 8s 600us/step - loss: 0.7251 - acc: 0.7860\n",
      "Epoch 439/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.7136 - acc: 0.7839\n",
      "Epoch 440/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.7029 - acc: 0.7880\n",
      "Epoch 441/500\n",
      "13783/13783 [==============================] - 8s 599us/step - loss: 0.7138 - acc: 0.7841\n",
      "Epoch 442/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.7064 - acc: 0.7850\n",
      "Epoch 443/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.7203 - acc: 0.7875\n",
      "Epoch 444/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.7151 - acc: 0.7854\n",
      "Epoch 445/500\n",
      "13783/13783 [==============================] - 8s 601us/step - loss: 0.6986 - acc: 0.7859\n",
      "Epoch 446/500\n",
      "13783/13783 [==============================] - 8s 599us/step - loss: 0.7040 - acc: 0.7905\n",
      "Epoch 447/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.7113 - acc: 0.7872\n",
      "Epoch 448/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.7040 - acc: 0.7889\n",
      "Epoch 449/500\n",
      "13783/13783 [==============================] - 9s 621us/step - loss: 0.6900 - acc: 0.7893\n",
      "Epoch 450/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.7042 - acc: 0.7877\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "look on his face when he heisted himself and took leave of this place through a hole in the smog without leaving a trace and all that the lorax left here in this mess was a small pile of rocks with one word unless whatever that meant well i just guess things just just pay it hurry on hop aboard that lazy said and ill scare what you know and that with beaches back then the far air beaches there\n",
      "Epoch 451/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.6958 - acc: 0.7921\n",
      "Epoch 452/500\n",
      "13783/13783 [==============================] - 8s 602us/step - loss: 0.6935 - acc: 0.7906\n",
      "Epoch 453/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.6896 - acc: 0.7965\n",
      "Epoch 454/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.6973 - acc: 0.7937\n",
      "Epoch 455/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.6898 - acc: 0.7925\n",
      "Epoch 456/500\n",
      "13783/13783 [==============================] - 8s 601us/step - loss: 0.6893 - acc: 0.7935\n",
      "Epoch 457/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.6741 - acc: 0.7986\n",
      "Epoch 458/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.6902 - acc: 0.7943\n",
      "Epoch 459/500\n",
      "13783/13783 [==============================] - 8s 598us/step - loss: 0.6801 - acc: 0.7967\n",
      "Epoch 460/500\n",
      "13783/13783 [==============================] - 8s 603us/step - loss: 0.6741 - acc: 0.8048\n",
      "Epoch 461/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.6852 - acc: 0.7930\n",
      "Epoch 462/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.6827 - acc: 0.8018\n",
      "Epoch 463/500\n",
      "13783/13783 [==============================] - 8s 599us/step - loss: 0.6688 - acc: 0.7980\n",
      "Epoch 464/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.6791 - acc: 0.7966\n",
      "Epoch 465/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.6732 - acc: 0.8013\n",
      "Epoch 466/500\n",
      "13783/13783 [==============================] - 8s 598us/step - loss: 0.6764 - acc: 0.7960\n",
      "Epoch 467/500\n",
      "13783/13783 [==============================] - 8s 601us/step - loss: 0.6723 - acc: 0.8011\n",
      "Epoch 468/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.6692 - acc: 0.8016\n",
      "Epoch 469/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.6673 - acc: 0.8034\n",
      "Epoch 470/500\n",
      "13783/13783 [==============================] - 8s 600us/step - loss: 0.6661 - acc: 0.8031\n",
      "Epoch 471/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.6840 - acc: 0.7947\n",
      "Epoch 472/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.6443 - acc: 0.8051\n",
      "Epoch 473/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.6710 - acc: 0.8001\n",
      "Epoch 474/500\n",
      "13783/13783 [==============================] - 8s 600us/step - loss: 0.6630 - acc: 0.8077\n",
      "Epoch 475/500\n",
      "13783/13783 [==============================] - 8s 600us/step - loss: 0.6579 - acc: 0.8046\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "far end of town where the grickle grass grows and the wind smells slow and sour when it blows and no birds ever sing excepting old crows is the street of the lifted lorax and deep in the grickle grass some people say if you look deep enough you can still see today where the lorax once stood just as long as it could before somebody lifted the lorax away what was the lorax and why was it there and\n",
      "Epoch 476/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.6790 - acc: 0.7992\n",
      "Epoch 477/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.6562 - acc: 0.8035\n",
      "Epoch 478/500\n",
      "13783/13783 [==============================] - 8s 598us/step - loss: 0.6618 - acc: 0.8024\n",
      "Epoch 479/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.6570 - acc: 0.8034\n",
      "Epoch 480/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.6729 - acc: 0.8031\n",
      "Epoch 481/500\n",
      "13783/13783 [==============================] - 8s 600us/step - loss: 0.6773 - acc: 0.7990\n",
      "Epoch 482/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.6553 - acc: 0.8025\n",
      "Epoch 483/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.6632 - acc: 0.8034\n",
      "Epoch 484/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.6516 - acc: 0.8069\n",
      "Epoch 485/500\n",
      "13783/13783 [==============================] - 8s 599us/step - loss: 0.6534 - acc: 0.8069\n",
      "Epoch 486/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.6538 - acc: 0.8041\n",
      "Epoch 487/500\n",
      "13783/13783 [==============================] - 8s 598us/step - loss: 0.6405 - acc: 0.8087\n",
      "Epoch 488/500\n",
      "13783/13783 [==============================] - 8s 598us/step - loss: 0.6551 - acc: 0.8061\n",
      "Epoch 489/500\n",
      "13783/13783 [==============================] - 8s 604us/step - loss: 0.6426 - acc: 0.8079\n",
      "Epoch 490/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.6313 - acc: 0.8112\n",
      "Epoch 491/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.6485 - acc: 0.8082\n",
      "Epoch 492/500\n",
      "13783/13783 [==============================] - 8s 603us/step - loss: 0.6227 - acc: 0.8095\n",
      "Epoch 493/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.6485 - acc: 0.8085\n",
      "Epoch 494/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.6452 - acc: 0.8061\n",
      "Epoch 495/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.6400 - acc: 0.8055\n",
      "Epoch 496/500\n",
      "13783/13783 [==============================] - 8s 600us/step - loss: 0.6464 - acc: 0.8055\n",
      "Epoch 497/500\n",
      "13783/13783 [==============================] - 8s 597us/step - loss: 0.6474 - acc: 0.8038\n",
      "Epoch 498/500\n",
      "13783/13783 [==============================] - 8s 595us/step - loss: 0.6466 - acc: 0.8057\n",
      "Epoch 499/500\n",
      "13783/13783 [==============================] - 8s 596us/step - loss: 0.6338 - acc: 0.8090\n",
      "Epoch 500/500\n",
      "13783/13783 [==============================] - 8s 599us/step - loss: 0.6545 - acc: 0.8064\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "places youll go youll be on y our way up youll be seeing great sights youll join the high fliers who soar to high heights you wont lag behind because youll have the speed youll pass the whole gang and youll soon take the lead wherever you fly youll be best of the best wherever you go you will top all the rest except when you dont because sometimes you wont im sorry to say so but sadly its true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9747bcf1d0>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, batch_size=256, epochs=500, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30571"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "js = \"[\"\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    js += '\"' + word + '\",'\n",
    "js += \"]\"\n",
    "f = open(TEXT_NAME + \"-js-tokenizer.txt\", \"w\")\n",
    "f.write(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "43796/43796 [==============================] - 31s 715us/step - loss: 5.9394 - acc: 0.0627\n",
      "Epoch 2/500\n",
      "43796/43796 [==============================] - 28s 650us/step - loss: 5.6464 - acc: 0.0766\n",
      "Epoch 3/500\n",
      "43796/43796 [==============================] - 28s 649us/step - loss: 5.4177 - acc: 0.0942\n",
      "Epoch 4/500\n",
      "43796/43796 [==============================] - 28s 645us/step - loss: 5.2886 - acc: 0.0972\n",
      "Epoch 5/500\n",
      "43796/43796 [==============================] - 28s 645us/step - loss: 5.1938 - acc: 0.0991\n",
      "Epoch 6/500\n",
      "43796/43796 [==============================] - 28s 642us/step - loss: 5.1158 - acc: 0.1031\n",
      "Epoch 7/500\n",
      "43796/43796 [==============================] - 28s 644us/step - loss: 5.0376 - acc: 0.1098\n",
      "Epoch 8/500\n",
      "43796/43796 [==============================] - 28s 643us/step - loss: 4.9587 - acc: 0.1168\n",
      "Epoch 9/500\n",
      "43796/43796 [==============================] - 28s 645us/step - loss: 4.8826 - acc: 0.1244\n",
      "Epoch 10/500\n",
      "43796/43796 [==============================] - 28s 643us/step - loss: 4.8180 - acc: 0.1293\n",
      "Epoch 11/500\n",
      "43796/43796 [==============================] - 28s 644us/step - loss: 4.7633 - acc: 0.1344\n",
      "Epoch 12/500\n",
      "43796/43796 [==============================] - 29s 658us/step - loss: 4.7143 - acc: 0.1371\n",
      "Epoch 13/500\n",
      "43796/43796 [==============================] - 28s 646us/step - loss: 4.6715 - acc: 0.1423\n",
      "Epoch 14/500\n",
      "43796/43796 [==============================] - 28s 645us/step - loss: 4.6288 - acc: 0.1472\n",
      "Epoch 15/500\n",
      "43796/43796 [==============================] - 28s 646us/step - loss: 4.5853 - acc: 0.1517\n",
      "Epoch 16/500\n",
      "43796/43796 [==============================] - 28s 643us/step - loss: 4.5488 - acc: 0.1561\n",
      "Epoch 17/500\n",
      "43796/43796 [==============================] - 28s 647us/step - loss: 4.5102 - acc: 0.1612\n",
      "Epoch 18/500\n",
      "43796/43796 [==============================] - 28s 644us/step - loss: 4.4774 - acc: 0.1628\n",
      "Epoch 19/500\n",
      "43796/43796 [==============================] - 28s 645us/step - loss: 4.4415 - acc: 0.1664\n",
      "Epoch 20/500\n",
      "43796/43796 [==============================] - 28s 646us/step - loss: 4.4068 - acc: 0.1710\n",
      "Epoch 21/500\n",
      "43796/43796 [==============================] - 28s 645us/step - loss: 4.3773 - acc: 0.1723\n",
      "Epoch 22/500\n",
      "43796/43796 [==============================] - 28s 647us/step - loss: 4.3493 - acc: 0.1752\n",
      "Epoch 23/500\n",
      "43796/43796 [==============================] - 29s 660us/step - loss: 4.3219 - acc: 0.1770\n",
      "Epoch 24/500\n",
      "43796/43796 [==============================] - 31s 702us/step - loss: 4.2968 - acc: 0.1796\n",
      "Epoch 25/500\n",
      "43796/43796 [==============================] - 34s 774us/step - loss: 4.2733 - acc: 0.1812\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "is you to be a great , and the lion was a great , and the scarecrow was a great , and the scarecrow was a great , and the scarecrow was a great , and the scarecrow was a great , and the scarecrow was the emerald city , and the scarecrow was a great , and the scarecrow was a great , and the scarecrow was a great , and the scarecrow was a great , and the\n",
      "Epoch 26/500\n",
      "43796/43796 [==============================] - 31s 714us/step - loss: 4.2479 - acc: 0.1841\n",
      "Epoch 27/500\n",
      "43796/43796 [==============================] - 35s 805us/step - loss: 4.2253 - acc: 0.1855\n",
      "Epoch 28/500\n",
      "43796/43796 [==============================] - 34s 771us/step - loss: 4.2022 - acc: 0.1882\n",
      "Epoch 29/500\n",
      "43796/43796 [==============================] - 29s 651us/step - loss: 4.1833 - acc: 0.1886\n",
      "Epoch 30/500\n",
      "43796/43796 [==============================] - 31s 711us/step - loss: 4.1616 - acc: 0.1918\n",
      "Epoch 31/500\n",
      "43796/43796 [==============================] - 30s 685us/step - loss: 4.1455 - acc: 0.1930\n",
      "Epoch 32/500\n",
      "43796/43796 [==============================] - 31s 705us/step - loss: 4.1261 - acc: 0.1970\n",
      "Epoch 33/500\n",
      "43796/43796 [==============================] - 34s 772us/step - loss: 4.1081 - acc: 0.1969\n",
      "Epoch 34/500\n",
      "43796/43796 [==============================] - 30s 678us/step - loss: 4.0883 - acc: 0.1991\n",
      "Epoch 35/500\n",
      "43796/43796 [==============================] - 31s 701us/step - loss: 4.0718 - acc: 0.2018\n",
      "Epoch 36/500\n",
      "43796/43796 [==============================] - 30s 680us/step - loss: 4.0548 - acc: 0.2029\n",
      "Epoch 37/500\n",
      "43796/43796 [==============================] - 33s 748us/step - loss: 4.0383 - acc: 0.2049\n",
      "Epoch 38/500\n",
      "43796/43796 [==============================] - 36s 815us/step - loss: 4.0217 - acc: 0.2069\n",
      "Epoch 39/500\n",
      "43796/43796 [==============================] - 33s 753us/step - loss: 4.0030 - acc: 0.2087\n",
      "Epoch 40/500\n",
      "43796/43796 [==============================] - 31s 707us/step - loss: 3.9895 - acc: 0.2100\n",
      "Epoch 41/500\n",
      "43796/43796 [==============================] - 32s 722us/step - loss: 3.9756 - acc: 0.2126\n",
      "Epoch 42/500\n",
      "43796/43796 [==============================] - 30s 683us/step - loss: 3.9594 - acc: 0.2134\n",
      "Epoch 43/500\n",
      "43796/43796 [==============================] - 35s 805us/step - loss: 3.9418 - acc: 0.2150\n",
      "Epoch 44/500\n",
      "43796/43796 [==============================] - 32s 737us/step - loss: 3.9270 - acc: 0.2174\n",
      "Epoch 45/500\n",
      "43796/43796 [==============================] - 35s 801us/step - loss: 3.9138 - acc: 0.2188\n",
      "Epoch 46/500\n",
      "43796/43796 [==============================] - 35s 797us/step - loss: 3.8983 - acc: 0.2201\n",
      "Epoch 47/500\n",
      "43796/43796 [==============================] - 31s 717us/step - loss: 3.8810 - acc: 0.2213\n",
      "Epoch 48/500\n",
      "43796/43796 [==============================] - 36s 818us/step - loss: 3.8709 - acc: 0.2226\n",
      "Epoch 49/500\n",
      "43796/43796 [==============================] - 33s 765us/step - loss: 3.8540 - acc: 0.2253\n",
      "Epoch 50/500\n",
      "43796/43796 [==============================] - 31s 707us/step - loss: 3.8450 - acc: 0.2264\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "the emerald city , and i am sure i am a great , said the scarecrow . i am sure i am a heart , said the scarecrow . i am sure i am a heart , said the scarecrow . i am sure i am a heart , said the scarecrow . i am sure i am a heart , said the scarecrow . i am sure i am a heart , said the scarecrow . i am sure\n",
      "Epoch 51/500\n",
      "43796/43796 [==============================] - 31s 711us/step - loss: 3.8284 - acc: 0.2281\n",
      "Epoch 52/500\n",
      "43796/43796 [==============================] - 31s 716us/step - loss: 3.8219 - acc: 0.2297\n",
      "Epoch 53/500\n",
      "43796/43796 [==============================] - 28s 636us/step - loss: 3.8020 - acc: 0.2311\n",
      "Epoch 54/500\n",
      "43796/43796 [==============================] - 28s 632us/step - loss: 3.7915 - acc: 0.2313\n",
      "Epoch 55/500\n",
      "43796/43796 [==============================] - 28s 631us/step - loss: 3.7828 - acc: 0.2355\n",
      "Epoch 56/500\n",
      "43796/43796 [==============================] - 28s 632us/step - loss: 3.7653 - acc: 0.2361\n",
      "Epoch 57/500\n",
      "43796/43796 [==============================] - 28s 632us/step - loss: 3.7563 - acc: 0.2367\n",
      "Epoch 58/500\n",
      "43796/43796 [==============================] - 28s 637us/step - loss: 3.7438 - acc: 0.2378\n",
      "Epoch 59/500\n",
      "43796/43796 [==============================] - 28s 631us/step - loss: 3.7355 - acc: 0.2410\n",
      "Epoch 60/500\n",
      "43796/43796 [==============================] - 28s 632us/step - loss: 3.7123 - acc: 0.2422\n",
      "Epoch 61/500\n",
      "43796/43796 [==============================] - 28s 632us/step - loss: 3.7078 - acc: 0.2438\n",
      "Epoch 62/500\n",
      "43796/43796 [==============================] - 28s 633us/step - loss: 3.6979 - acc: 0.2447\n",
      "Epoch 63/500\n",
      "43796/43796 [==============================] - 28s 633us/step - loss: 3.6873 - acc: 0.2471\n",
      "Epoch 64/500\n",
      "43796/43796 [==============================] - 28s 634us/step - loss: 3.6715 - acc: 0.2492\n",
      "Epoch 65/500\n",
      "43796/43796 [==============================] - 28s 631us/step - loss: 3.6649 - acc: 0.2479\n",
      "Epoch 66/500\n",
      "43796/43796 [==============================] - 28s 633us/step - loss: 3.6540 - acc: 0.2508\n",
      "Epoch 67/500\n",
      "43796/43796 [==============================] - 28s 631us/step - loss: 3.6445 - acc: 0.2510\n",
      "Epoch 68/500\n",
      "43796/43796 [==============================] - 28s 630us/step - loss: 3.6278 - acc: 0.2553\n",
      "Epoch 69/500\n",
      "43796/43796 [==============================] - 28s 632us/step - loss: 3.6166 - acc: 0.2537\n",
      "Epoch 70/500\n",
      "43796/43796 [==============================] - 28s 632us/step - loss: 3.6098 - acc: 0.2561\n",
      "Epoch 71/500\n",
      "43796/43796 [==============================] - 28s 631us/step - loss: 3.5995 - acc: 0.2573\n",
      "Epoch 72/500\n",
      "43796/43796 [==============================] - 28s 630us/step - loss: 3.5942 - acc: 0.2588\n",
      "Epoch 73/500\n",
      "43796/43796 [==============================] - 28s 632us/step - loss: 3.5859 - acc: 0.2604\n",
      "Epoch 74/500\n",
      "43796/43796 [==============================] - 28s 630us/step - loss: 3.5768 - acc: 0.2632\n",
      "Epoch 75/500\n",
      "43796/43796 [==============================] - 28s 632us/step - loss: 3.5597 - acc: 0.2633\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      ", said the scarecrow . i am sure i am sure i am sure i am sure i am sure i am sure i am sure i am sure i am sure i am sure i am sure i am sure i am sure i am sure i am sure i am sure i am sure i am sure i am sure i am sure i am sure i am sure i am sure i am sure i am sure\n",
      "Epoch 76/500\n",
      "43796/43796 [==============================] - 28s 631us/step - loss: 3.5509 - acc: 0.2656\n",
      "Epoch 77/500\n",
      "43796/43796 [==============================] - 28s 632us/step - loss: 3.5442 - acc: 0.2669\n",
      "Epoch 78/500\n",
      "43796/43796 [==============================] - 28s 630us/step - loss: 3.5298 - acc: 0.2677\n",
      "Epoch 79/500\n",
      "43796/43796 [==============================] - 28s 630us/step - loss: 3.5247 - acc: 0.2680\n",
      "Epoch 80/500\n",
      "43796/43796 [==============================] - 28s 630us/step - loss: 3.5168 - acc: 0.2720\n",
      "Epoch 81/500\n",
      "43796/43796 [==============================] - 28s 630us/step - loss: 3.5100 - acc: 0.2734\n",
      "Epoch 82/500\n",
      "43796/43796 [==============================] - 28s 632us/step - loss: 3.5048 - acc: 0.2723\n",
      "Epoch 83/500\n",
      "43796/43796 [==============================] - 28s 630us/step - loss: 3.4900 - acc: 0.2759\n",
      "Epoch 84/500\n",
      "43796/43796 [==============================] - 28s 630us/step - loss: 3.4776 - acc: 0.2765\n",
      "Epoch 85/500\n",
      "43796/43796 [==============================] - 28s 634us/step - loss: 3.4712 - acc: 0.2769\n",
      "Epoch 86/500\n",
      "43796/43796 [==============================] - 28s 633us/step - loss: 3.4676 - acc: 0.2786\n",
      "Epoch 87/500\n",
      "43796/43796 [==============================] - 28s 632us/step - loss: 3.4591 - acc: 0.2808\n",
      "Epoch 88/500\n",
      "43796/43796 [==============================] - 28s 632us/step - loss: 3.4456 - acc: 0.2838\n",
      "Epoch 89/500\n",
      "43796/43796 [==============================] - 28s 632us/step - loss: 3.4402 - acc: 0.2823\n",
      "Epoch 90/500\n",
      "43796/43796 [==============================] - 28s 632us/step - loss: 3.4329 - acc: 0.2845\n",
      "Epoch 91/500\n",
      "43796/43796 [==============================] - 28s 630us/step - loss: 3.4277 - acc: 0.2869\n",
      "Epoch 92/500\n",
      "43796/43796 [==============================] - 28s 631us/step - loss: 3.4108 - acc: 0.2884\n",
      "Epoch 93/500\n",
      "43796/43796 [==============================] - 28s 631us/step - loss: 3.4127 - acc: 0.2899\n",
      "Epoch 94/500\n",
      "43796/43796 [==============================] - 28s 631us/step - loss: 3.4084 - acc: 0.2912\n",
      "Epoch 95/500\n",
      "43796/43796 [==============================] - 28s 633us/step - loss: 3.4018 - acc: 0.2906\n",
      "Epoch 96/500\n",
      "43796/43796 [==============================] - 28s 633us/step - loss: 3.3852 - acc: 0.2955\n",
      "Epoch 97/500\n",
      "43796/43796 [==============================] - 28s 632us/step - loss: 3.3825 - acc: 0.2925\n",
      "Epoch 98/500\n",
      "43796/43796 [==============================] - 28s 631us/step - loss: 3.3672 - acc: 0.2959\n",
      "Epoch 99/500\n",
      "43796/43796 [==============================] - 28s 633us/step - loss: 3.3643 - acc: 0.2968\n",
      "Epoch 100/500\n",
      "43796/43796 [==============================] - 28s 632us/step - loss: 3.3577 - acc: 0.2956\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "saw the other woman came to the trees and had come down from his head , for he was so by the scarecrow morning , who was quite very , and soon toto had been so a small , or a big thing in her small grass , and found a little thing which had been made it in her head . there were a while the scarecrow had blue in three eyes , and made a green with green\n",
      "Epoch 101/500\n",
      "43796/43796 [==============================] - 28s 632us/step - loss: 3.3520 - acc: 0.2984\n",
      "Epoch 102/500\n",
      "43796/43796 [==============================] - 28s 631us/step - loss: 3.3454 - acc: 0.3030\n",
      "Epoch 103/500\n",
      "43796/43796 [==============================] - 28s 632us/step - loss: 3.3404 - acc: 0.3002\n",
      "Epoch 104/500\n",
      "43796/43796 [==============================] - 28s 631us/step - loss: 3.3356 - acc: 0.3000\n",
      "Epoch 105/500\n",
      "43796/43796 [==============================] - 28s 632us/step - loss: 3.3244 - acc: 0.3044\n",
      "Epoch 106/500\n",
      "43796/43796 [==============================] - 28s 632us/step - loss: 3.3169 - acc: 0.3062\n",
      "Epoch 107/500\n",
      "43796/43796 [==============================] - 28s 633us/step - loss: 3.3120 - acc: 0.3070\n",
      "Epoch 108/500\n",
      "43796/43796 [==============================] - 28s 634us/step - loss: 3.3080 - acc: 0.3064\n",
      "Epoch 109/500\n",
      "43796/43796 [==============================] - 28s 631us/step - loss: 3.2977 - acc: 0.3086\n",
      "Epoch 110/500\n",
      "43796/43796 [==============================] - 28s 632us/step - loss: 3.2930 - acc: 0.3109\n",
      "Epoch 111/500\n",
      "43796/43796 [==============================] - 28s 633us/step - loss: 3.2803 - acc: 0.3118\n",
      "Epoch 112/500\n",
      "43796/43796 [==============================] - 28s 633us/step - loss: 3.2807 - acc: 0.3123\n",
      "Epoch 113/500\n",
      "43796/43796 [==============================] - 28s 632us/step - loss: 3.2715 - acc: 0.3154\n",
      "Epoch 114/500\n",
      "43796/43796 [==============================] - 28s 632us/step - loss: 3.2668 - acc: 0.3148\n",
      "Epoch 115/500\n",
      "43796/43796 [==============================] - 28s 632us/step - loss: 3.2629 - acc: 0.3181\n",
      "Epoch 116/500\n",
      "43796/43796 [==============================] - 28s 633us/step - loss: 3.2557 - acc: 0.3178\n",
      "Epoch 117/500\n",
      "43796/43796 [==============================] - 28s 630us/step - loss: 3.2523 - acc: 0.3184\n",
      "Epoch 118/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 3.2377 - acc: 0.3189\n",
      "Epoch 119/500\n",
      "43796/43796 [==============================] - 27s 618us/step - loss: 3.2352 - acc: 0.3228\n",
      "Epoch 120/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 3.2295 - acc: 0.3211\n",
      "Epoch 121/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 3.2309 - acc: 0.3211\n",
      "Epoch 122/500\n",
      "43796/43796 [==============================] - 27s 615us/step - loss: 3.2232 - acc: 0.3250\n",
      "Epoch 123/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 3.2153 - acc: 0.3264\n",
      "Epoch 124/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 3.2013 - acc: 0.3276\n",
      "Epoch 125/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 3.2035 - acc: 0.3287\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "other woman , who had been too much , who has the other woman was so much so that the travelers had made the as green and from a big head , which seemed with up as after the sun , and which made it many and thick in the sun , like a big voice that had here at them about more one day , who was so made with straw with a green with fell with a small\n",
      "Epoch 126/500\n",
      "43796/43796 [==============================] - 27s 615us/step - loss: 3.1978 - acc: 0.3287\n",
      "Epoch 127/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 3.1948 - acc: 0.3283\n",
      "Epoch 128/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 3.1853 - acc: 0.3306\n",
      "Epoch 129/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 3.1823 - acc: 0.3317\n",
      "Epoch 130/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 3.1729 - acc: 0.3325\n",
      "Epoch 131/500\n",
      "43796/43796 [==============================] - 27s 615us/step - loss: 3.1703 - acc: 0.3370\n",
      "Epoch 132/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 3.1612 - acc: 0.3352\n",
      "Epoch 133/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 3.1540 - acc: 0.3383\n",
      "Epoch 134/500\n",
      "43796/43796 [==============================] - 27s 618us/step - loss: 3.1570 - acc: 0.3371\n",
      "Epoch 135/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 3.1481 - acc: 0.3390\n",
      "Epoch 136/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 3.1436 - acc: 0.3410\n",
      "Epoch 137/500\n",
      "43796/43796 [==============================] - 27s 615us/step - loss: 3.1446 - acc: 0.3404\n",
      "Epoch 138/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 3.1316 - acc: 0.3433\n",
      "Epoch 139/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43796/43796 [==============================] - 27s 618us/step - loss: 3.1310 - acc: 0.3424\n",
      "Epoch 140/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 3.1323 - acc: 0.3438\n",
      "Epoch 141/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 3.1208 - acc: 0.3460\n",
      "Epoch 142/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 3.1179 - acc: 0.3450\n",
      "Epoch 143/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 3.1114 - acc: 0.3486\n",
      "Epoch 144/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 3.1139 - acc: 0.3469\n",
      "Epoch 145/500\n",
      "43796/43796 [==============================] - 27s 618us/step - loss: 3.1015 - acc: 0.3494\n",
      "Epoch 146/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 3.0936 - acc: 0.3507\n",
      "Epoch 147/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 3.0910 - acc: 0.3477\n",
      "Epoch 148/500\n",
      "43796/43796 [==============================] - 27s 618us/step - loss: 3.0917 - acc: 0.3503\n",
      "Epoch 149/500\n",
      "43796/43796 [==============================] - 27s 615us/step - loss: 3.0836 - acc: 0.3521\n",
      "Epoch 150/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 3.0790 - acc: 0.3533\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "am sure how are we have been an other way to us , said dorothy . i am not right to get back to kansas . so that im so much happy , answered the scarecrow . the scarecrow is so much so if i should tell you asked dorothy . no one so that i have been afraid , replied dorothy . oh , yes asked dorothy . aunt em is very good beautiful way to give me courage\n",
      "Epoch 151/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 3.0726 - acc: 0.3535\n",
      "Epoch 152/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 3.0719 - acc: 0.3533\n",
      "Epoch 153/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 3.0642 - acc: 0.3550\n",
      "Epoch 154/500\n",
      "43796/43796 [==============================] - 27s 618us/step - loss: 3.0537 - acc: 0.3590\n",
      "Epoch 155/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 3.0488 - acc: 0.3566\n",
      "Epoch 156/500\n",
      "43796/43796 [==============================] - 27s 618us/step - loss: 3.0478 - acc: 0.3577\n",
      "Epoch 157/500\n",
      "43796/43796 [==============================] - 27s 615us/step - loss: 3.0529 - acc: 0.3585\n",
      "Epoch 158/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 3.0408 - acc: 0.3625\n",
      "Epoch 159/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 3.0454 - acc: 0.3619\n",
      "Epoch 160/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 3.0328 - acc: 0.3642\n",
      "Epoch 161/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 3.0333 - acc: 0.3606\n",
      "Epoch 162/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 3.0306 - acc: 0.3654\n",
      "Epoch 163/500\n",
      "43796/43796 [==============================] - 27s 618us/step - loss: 3.0219 - acc: 0.3624\n",
      "Epoch 164/500\n",
      "43796/43796 [==============================] - 27s 618us/step - loss: 3.0056 - acc: 0.3672\n",
      "Epoch 165/500\n",
      "43796/43796 [==============================] - 27s 618us/step - loss: 3.0128 - acc: 0.3656\n",
      "Epoch 166/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 3.0010 - acc: 0.3677\n",
      "Epoch 167/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 3.0008 - acc: 0.3675\n",
      "Epoch 168/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.9952 - acc: 0.3697\n",
      "Epoch 169/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 2.9968 - acc: 0.3695\n",
      "Epoch 170/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 2.9937 - acc: 0.3712\n",
      "Epoch 171/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 2.9824 - acc: 0.3714\n",
      "Epoch 172/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 2.9826 - acc: 0.3735\n",
      "Epoch 173/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 2.9798 - acc: 0.3708\n",
      "Epoch 174/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 2.9753 - acc: 0.3750\n",
      "Epoch 175/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.9757 - acc: 0.3766\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "friends were quite much greatly by the people or any of them , nothing , here they stood into the river . after she saw nothing more their eyes that when she made him their mind he asked . what can we do how we am such happy , for i have been done for my day , and then i am sure oz oh , yes , answered oz . why do you give me courage , answered the\n",
      "Epoch 176/500\n",
      "43796/43796 [==============================] - 27s 618us/step - loss: 2.9639 - acc: 0.3767\n",
      "Epoch 177/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 2.9634 - acc: 0.3771\n",
      "Epoch 178/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.9666 - acc: 0.3749\n",
      "Epoch 179/500\n",
      "43796/43796 [==============================] - 27s 618us/step - loss: 2.9526 - acc: 0.3773\n",
      "Epoch 180/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.9481 - acc: 0.3792\n",
      "Epoch 181/500\n",
      "43796/43796 [==============================] - 27s 618us/step - loss: 2.9534 - acc: 0.3800\n",
      "Epoch 182/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 2.9453 - acc: 0.3796\n",
      "Epoch 183/500\n",
      "43796/43796 [==============================] - 27s 618us/step - loss: 2.9496 - acc: 0.3764\n",
      "Epoch 184/500\n",
      "43796/43796 [==============================] - 27s 619us/step - loss: 2.9304 - acc: 0.3825\n",
      "Epoch 185/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 2.9357 - acc: 0.3816\n",
      "Epoch 186/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 2.9289 - acc: 0.3848\n",
      "Epoch 187/500\n",
      "43796/43796 [==============================] - 27s 618us/step - loss: 2.9301 - acc: 0.3825\n",
      "Epoch 188/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.9182 - acc: 0.3854\n",
      "Epoch 189/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 2.9142 - acc: 0.3859\n",
      "Epoch 190/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.9084 - acc: 0.3876\n",
      "Epoch 191/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.9110 - acc: 0.3854\n",
      "Epoch 192/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.9039 - acc: 0.3860\n",
      "Epoch 193/500\n",
      "43796/43796 [==============================] - 27s 615us/step - loss: 2.9019 - acc: 0.3865\n",
      "Epoch 194/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 2.8931 - acc: 0.3889\n",
      "Epoch 195/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.8990 - acc: 0.3874\n",
      "Epoch 196/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.8869 - acc: 0.3906\n",
      "Epoch 197/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.8958 - acc: 0.3902\n",
      "Epoch 198/500\n",
      "43796/43796 [==============================] - 27s 615us/step - loss: 2.8766 - acc: 0.3938\n",
      "Epoch 199/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.8846 - acc: 0.3925\n",
      "Epoch 200/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.8831 - acc: 0.3930\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "through the air and looked at the ground . dorothy did not know i could be such to do what he walked back to oz again , with toto in their eyes before her . we will go back , he having been quite much at all the end of the forest and toto had been too afraid . but he could be quite much at one of the lion ever been killed the wicked witch of the west ,\n",
      "Epoch 201/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.8764 - acc: 0.3951\n",
      "Epoch 202/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.8773 - acc: 0.3949\n",
      "Epoch 203/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 2.8747 - acc: 0.3982\n",
      "Epoch 204/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.8646 - acc: 0.3962\n",
      "Epoch 205/500\n",
      "43796/43796 [==============================] - 27s 618us/step - loss: 2.8574 - acc: 0.3941\n",
      "Epoch 206/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 2.8606 - acc: 0.3955\n",
      "Epoch 207/500\n",
      "43796/43796 [==============================] - 27s 618us/step - loss: 2.8559 - acc: 0.3973\n",
      "Epoch 208/500\n",
      "43796/43796 [==============================] - 27s 615us/step - loss: 2.8575 - acc: 0.3956\n",
      "Epoch 209/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.8466 - acc: 0.3980\n",
      "Epoch 210/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.8536 - acc: 0.3996\n",
      "Epoch 211/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.8428 - acc: 0.4007\n",
      "Epoch 212/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 2.8360 - acc: 0.4003\n",
      "Epoch 213/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.8279 - acc: 0.4023\n",
      "Epoch 214/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.8354 - acc: 0.4028\n",
      "Epoch 215/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 2.8294 - acc: 0.4044\n",
      "Epoch 216/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 2.8248 - acc: 0.4038\n",
      "Epoch 217/500\n",
      "43796/43796 [==============================] - 27s 618us/step - loss: 2.8205 - acc: 0.4042\n",
      "Epoch 218/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.8153 - acc: 0.4066\n",
      "Epoch 219/500\n",
      "43796/43796 [==============================] - 27s 618us/step - loss: 2.8189 - acc: 0.4055\n",
      "Epoch 220/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 2.8134 - acc: 0.4056\n",
      "Epoch 221/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.8041 - acc: 0.4063\n",
      "Epoch 222/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.8111 - acc: 0.4061\n",
      "Epoch 223/500\n",
      "43796/43796 [==============================] - 27s 618us/step - loss: 2.7973 - acc: 0.4097\n",
      "Epoch 224/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.8003 - acc: 0.4080\n",
      "Epoch 225/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.7914 - acc: 0.4087\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "be an man , said the lion . when he is the same of the emerald city is very here , said oz , if i dont just my journey , remarked the scarecrow . i am sure it will make them as as as the winkies and toto so by the mice thought she could go . so that if i should never be afraid . what do you wish i thought we are very kind to kill the\n",
      "Epoch 226/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.7971 - acc: 0.4083\n",
      "Epoch 227/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.7980 - acc: 0.4086\n",
      "Epoch 228/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.7923 - acc: 0.4110\n",
      "Epoch 229/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 2.7857 - acc: 0.4108\n",
      "Epoch 230/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.7786 - acc: 0.4121\n",
      "Epoch 231/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 2.7854 - acc: 0.4120\n",
      "Epoch 232/500\n",
      "43796/43796 [==============================] - 27s 617us/step - loss: 2.7770 - acc: 0.4105\n",
      "Epoch 233/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.7699 - acc: 0.4132\n",
      "Epoch 234/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.7741 - acc: 0.4145\n",
      "Epoch 235/500\n",
      "43796/43796 [==============================] - 27s 615us/step - loss: 2.7729 - acc: 0.4137\n",
      "Epoch 236/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.7582 - acc: 0.4153\n",
      "Epoch 237/500\n",
      "43796/43796 [==============================] - 27s 616us/step - loss: 2.7542 - acc: 0.4145\n",
      "Epoch 238/500\n",
      "43796/43796 [==============================] - 27s 615us/step - loss: 2.7585 - acc: 0.4125\n",
      "Epoch 239/500\n",
      "43796/43796 [==============================] - 30s 683us/step - loss: 2.7521 - acc: 0.4189\n",
      "Epoch 240/500\n",
      "43796/43796 [==============================] - 29s 666us/step - loss: 2.7592 - acc: 0.4149\n",
      "Epoch 241/500\n",
      "43796/43796 [==============================] - 30s 679us/step - loss: 2.7497 - acc: 0.4189\n",
      "Epoch 242/500\n",
      "43796/43796 [==============================] - 31s 699us/step - loss: 2.7440 - acc: 0.4192\n",
      "Epoch 243/500\n",
      "43796/43796 [==============================] - 30s 677us/step - loss: 2.7412 - acc: 0.4195\n",
      "Epoch 244/500\n",
      "43796/43796 [==============================] - 30s 693us/step - loss: 2.7298 - acc: 0.4209\n",
      "Epoch 245/500\n",
      "43796/43796 [==============================] - 28s 648us/step - loss: 2.7402 - acc: 0.4188\n",
      "Epoch 246/500\n",
      "43796/43796 [==============================] - 28s 644us/step - loss: 2.7325 - acc: 0.4193\n",
      "Epoch 247/500\n",
      "43796/43796 [==============================] - 28s 644us/step - loss: 2.7448 - acc: 0.4173\n",
      "Epoch 248/500\n",
      "43796/43796 [==============================] - 28s 644us/step - loss: 2.7324 - acc: 0.4221\n",
      "Epoch 249/500\n",
      "43796/43796 [==============================] - 28s 646us/step - loss: 2.7248 - acc: 0.4233\n",
      "Epoch 250/500\n",
      "43796/43796 [==============================] - 28s 645us/step - loss: 2.7183 - acc: 0.4223\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "again , for he was quite much himself , for she had too afraid to kansas here , i cannot go with me . i am sure he would go back to some some little thing that far far that it was a bad thing how i thought of you , and he all walked along the winged monkeys nor a big man that i shall be very unhappy , and why do you wish to think how you can\n",
      "Epoch 251/500\n",
      "43796/43796 [==============================] - 28s 644us/step - loss: 2.7177 - acc: 0.4227\n",
      "Epoch 252/500\n",
      "43796/43796 [==============================] - 29s 655us/step - loss: 2.7252 - acc: 0.4236\n",
      "Epoch 253/500\n",
      "43796/43796 [==============================] - 28s 645us/step - loss: 2.7113 - acc: 0.4257\n",
      "Epoch 254/500\n",
      "43796/43796 [==============================] - 28s 646us/step - loss: 2.7127 - acc: 0.4228\n",
      "Epoch 255/500\n",
      "43796/43796 [==============================] - 29s 673us/step - loss: 2.7156 - acc: 0.4217\n",
      "Epoch 256/500\n",
      "43796/43796 [==============================] - 28s 644us/step - loss: 2.7071 - acc: 0.4214\n",
      "Epoch 257/500\n",
      "43796/43796 [==============================] - 28s 646us/step - loss: 2.7039 - acc: 0.4242\n",
      "Epoch 258/500\n",
      "43796/43796 [==============================] - 28s 645us/step - loss: 2.6953 - acc: 0.4242\n",
      "Epoch 259/500\n",
      "43796/43796 [==============================] - 28s 645us/step - loss: 2.6963 - acc: 0.4257\n",
      "Epoch 260/500\n",
      "43796/43796 [==============================] - 28s 644us/step - loss: 2.6974 - acc: 0.4279\n",
      "Epoch 261/500\n",
      "43796/43796 [==============================] - 28s 643us/step - loss: 2.6990 - acc: 0.4276\n",
      "Epoch 262/500\n",
      "43796/43796 [==============================] - 28s 645us/step - loss: 2.6926 - acc: 0.4267\n",
      "Epoch 263/500\n",
      "43796/43796 [==============================] - 28s 645us/step - loss: 2.6884 - acc: 0.4270\n",
      "Epoch 264/500\n",
      "43796/43796 [==============================] - 28s 644us/step - loss: 2.6707 - acc: 0.4309\n",
      "Epoch 265/500\n",
      "43796/43796 [==============================] - 28s 643us/step - loss: 2.6756 - acc: 0.4317\n",
      "Epoch 266/500\n",
      "43796/43796 [==============================] - 28s 649us/step - loss: 2.6804 - acc: 0.4317\n",
      "Epoch 267/500\n",
      "43796/43796 [==============================] - 28s 644us/step - loss: 2.6896 - acc: 0.4286\n",
      "Epoch 268/500\n",
      "43796/43796 [==============================] - 28s 644us/step - loss: 2.6806 - acc: 0.4298\n",
      "Epoch 269/500\n",
      "43796/43796 [==============================] - 28s 644us/step - loss: 2.6733 - acc: 0.4355\n",
      "Epoch 270/500\n",
      "43796/43796 [==============================] - 28s 645us/step - loss: 2.6652 - acc: 0.4335\n",
      "Epoch 271/500\n",
      "43796/43796 [==============================] - 28s 645us/step - loss: 2.6691 - acc: 0.4338\n",
      "Epoch 272/500\n",
      "43796/43796 [==============================] - 28s 644us/step - loss: 2.6575 - acc: 0.4355\n",
      "Epoch 273/500\n",
      "43796/43796 [==============================] - 28s 643us/step - loss: 2.6650 - acc: 0.4342\n",
      "Epoch 274/500\n",
      "43796/43796 [==============================] - 28s 646us/step - loss: 2.6626 - acc: 0.4341\n",
      "Epoch 275/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43796/43796 [==============================] - 28s 638us/step - loss: 2.6573 - acc: 0.4351\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "next morning they came the golden cap where the wicked witch had carried her would do next . this was only one much made him many years but this would have come into the world , for if i am more right that i am only a friend , although i am not so a great man . it was been so than he said . after out here the first time dorothy answered . if i am a humbug\n",
      "Epoch 276/500\n",
      "43796/43796 [==============================] - 28s 639us/step - loss: 2.6522 - acc: 0.4342\n",
      "Epoch 277/500\n",
      "43796/43796 [==============================] - 28s 639us/step - loss: 2.6521 - acc: 0.4355\n",
      "Epoch 278/500\n",
      "43796/43796 [==============================] - 28s 638us/step - loss: 2.6641 - acc: 0.4326\n",
      "Epoch 279/500\n",
      "43796/43796 [==============================] - 28s 639us/step - loss: 2.6498 - acc: 0.4352\n",
      "Epoch 280/500\n",
      "43796/43796 [==============================] - 28s 641us/step - loss: 2.6578 - acc: 0.4362\n",
      "Epoch 281/500\n",
      "43796/43796 [==============================] - 28s 638us/step - loss: 2.6387 - acc: 0.4378\n",
      "Epoch 282/500\n",
      "43796/43796 [==============================] - 28s 639us/step - loss: 2.6404 - acc: 0.4377\n",
      "Epoch 283/500\n",
      "43796/43796 [==============================] - 28s 640us/step - loss: 2.6411 - acc: 0.4388\n",
      "Epoch 284/500\n",
      "43796/43796 [==============================] - 28s 639us/step - loss: 2.6320 - acc: 0.4388\n",
      "Epoch 285/500\n",
      "43796/43796 [==============================] - 28s 639us/step - loss: 2.6351 - acc: 0.4406\n",
      "Epoch 286/500\n",
      "43796/43796 [==============================] - 28s 639us/step - loss: 2.6291 - acc: 0.4399\n",
      "Epoch 287/500\n",
      "43796/43796 [==============================] - 28s 640us/step - loss: 2.6327 - acc: 0.4413\n",
      "Epoch 288/500\n",
      "43796/43796 [==============================] - 28s 638us/step - loss: 2.6224 - acc: 0.4413\n",
      "Epoch 289/500\n",
      "43796/43796 [==============================] - 28s 640us/step - loss: 2.6296 - acc: 0.4408\n",
      "Epoch 290/500\n",
      "43796/43796 [==============================] - 28s 639us/step - loss: 2.6230 - acc: 0.4411\n",
      "Epoch 291/500\n",
      "43796/43796 [==============================] - 28s 639us/step - loss: 2.6169 - acc: 0.4431\n",
      "Epoch 292/500\n",
      "43796/43796 [==============================] - 28s 641us/step - loss: 2.6267 - acc: 0.4422\n",
      "Epoch 293/500\n",
      "43796/43796 [==============================] - 28s 637us/step - loss: 2.6155 - acc: 0.4427\n",
      "Epoch 294/500\n",
      "43796/43796 [==============================] - 28s 643us/step - loss: 2.6152 - acc: 0.4420\n",
      "Epoch 295/500\n",
      "43796/43796 [==============================] - 28s 639us/step - loss: 2.6118 - acc: 0.4419\n",
      "Epoch 296/500\n",
      "43796/43796 [==============================] - 28s 642us/step - loss: 2.6135 - acc: 0.4415\n",
      "Epoch 297/500\n",
      "43796/43796 [==============================] - 28s 639us/step - loss: 2.6081 - acc: 0.4449\n",
      "Epoch 298/500\n",
      "43796/43796 [==============================] - 28s 638us/step - loss: 2.6089 - acc: 0.4429\n",
      "Epoch 299/500\n",
      "43796/43796 [==============================] - 28s 639us/step - loss: 2.6017 - acc: 0.4452\n",
      "Epoch 300/500\n",
      "43796/43796 [==============================] - 28s 641us/step - loss: 2.6073 - acc: 0.4417\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "take me some aunt em being so much too too made the way to get back to kansas . when he did nothing out and a new place near the wonderful gray time the wicked witch being too too good a good beast , and if you will take me to keep me my way to be very beautiful . therefore i should have no brains good one who may have not much much at these people , i shall\n",
      "Epoch 301/500\n",
      "43796/43796 [==============================] - 28s 639us/step - loss: 2.5920 - acc: 0.4463\n",
      "Epoch 302/500\n",
      "43796/43796 [==============================] - 28s 639us/step - loss: 2.5911 - acc: 0.4489\n",
      "Epoch 303/500\n",
      "43796/43796 [==============================] - 28s 639us/step - loss: 2.5830 - acc: 0.4494\n",
      "Epoch 304/500\n",
      "43796/43796 [==============================] - 28s 641us/step - loss: 2.5931 - acc: 0.4469\n",
      "Epoch 305/500\n",
      "43796/43796 [==============================] - 28s 638us/step - loss: 2.5861 - acc: 0.4474\n",
      "Epoch 306/500\n",
      "43796/43796 [==============================] - 28s 641us/step - loss: 2.5820 - acc: 0.4475\n",
      "Epoch 307/500\n",
      "43796/43796 [==============================] - 28s 639us/step - loss: 2.5857 - acc: 0.4490\n",
      "Epoch 308/500\n",
      "43796/43796 [==============================] - 28s 637us/step - loss: 2.5772 - acc: 0.4472\n",
      "Epoch 309/500\n",
      "43796/43796 [==============================] - 28s 639us/step - loss: 2.5742 - acc: 0.4494\n",
      "Epoch 310/500\n",
      "43796/43796 [==============================] - 28s 638us/step - loss: 2.5840 - acc: 0.4511\n",
      "Epoch 311/500\n",
      "43796/43796 [==============================] - 28s 638us/step - loss: 2.5742 - acc: 0.4515\n",
      "Epoch 312/500\n",
      "43796/43796 [==============================] - 28s 638us/step - loss: 2.5740 - acc: 0.4513\n",
      "Epoch 313/500\n",
      "43796/43796 [==============================] - 28s 639us/step - loss: 2.5656 - acc: 0.4498\n",
      "Epoch 314/500\n",
      "43796/43796 [==============================] - 28s 639us/step - loss: 2.5596 - acc: 0.4513\n",
      "Epoch 315/500\n",
      "43796/43796 [==============================] - 28s 640us/step - loss: 2.5685 - acc: 0.4502\n",
      "Epoch 316/500\n",
      "43796/43796 [==============================] - 28s 640us/step - loss: 2.5548 - acc: 0.4533\n",
      "Epoch 317/500\n",
      "43796/43796 [==============================] - 28s 641us/step - loss: 2.5627 - acc: 0.4526\n",
      "Epoch 318/500\n",
      "43796/43796 [==============================] - 28s 639us/step - loss: 2.5546 - acc: 0.4511\n",
      "Epoch 319/500\n",
      "43796/43796 [==============================] - 28s 639us/step - loss: 2.5535 - acc: 0.4550\n",
      "Epoch 320/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.5500 - acc: 0.4549\n",
      "Epoch 321/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.5564 - acc: 0.4511\n",
      "Epoch 322/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.5441 - acc: 0.4534\n",
      "Epoch 323/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.5371 - acc: 0.4539\n",
      "Epoch 324/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.5346 - acc: 0.4573\n",
      "Epoch 325/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.5363 - acc: 0.4552\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "she could not hurt it . there were no first and i am really friend , said the lion , for she might have also left carefully . this just was quite than at all many , for he had always ever so happy , for if we may never ever know anything , said dorothy , who has been too much , so that you know what she could be too afraid . if dorothy one of us will\n",
      "Epoch 326/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.5344 - acc: 0.4545\n",
      "Epoch 327/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.5356 - acc: 0.4563\n",
      "Epoch 328/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.5230 - acc: 0.4563\n",
      "Epoch 329/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.5328 - acc: 0.4573\n",
      "Epoch 330/500\n",
      "43796/43796 [==============================] - 27s 626us/step - loss: 2.5292 - acc: 0.4565\n",
      "Epoch 331/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.5270 - acc: 0.4583\n",
      "Epoch 332/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.5207 - acc: 0.4594\n",
      "Epoch 333/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.5171 - acc: 0.4595\n",
      "Epoch 334/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.5148 - acc: 0.4618\n",
      "Epoch 335/500\n",
      "43796/43796 [==============================] - 27s 626us/step - loss: 2.5270 - acc: 0.4558\n",
      "Epoch 336/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.5158 - acc: 0.4599\n",
      "Epoch 337/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.5115 - acc: 0.4578\n",
      "Epoch 338/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.5101 - acc: 0.4620\n",
      "Epoch 339/500\n",
      "43796/43796 [==============================] - 27s 625us/step - loss: 2.5074 - acc: 0.4621\n",
      "Epoch 340/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.5057 - acc: 0.4609\n",
      "Epoch 341/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.5117 - acc: 0.4606\n",
      "Epoch 342/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.5034 - acc: 0.4629\n",
      "Epoch 343/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.5076 - acc: 0.4597\n",
      "Epoch 344/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.5061 - acc: 0.4623\n",
      "Epoch 345/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.4870 - acc: 0.4645\n",
      "Epoch 346/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.4945 - acc: 0.4636\n",
      "Epoch 347/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.4855 - acc: 0.4616\n",
      "Epoch 348/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.4963 - acc: 0.4624\n",
      "Epoch 349/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.4861 - acc: 0.4628\n",
      "Epoch 350/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.4831 - acc: 0.4636\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "girl anxiously . what is he asked dorothy . are go upon the beautiful castle i am more very being for the great oz , and then i should tell you back asked dorothy . no one would come to me again . i am a humbug , said the scarecrow , when oz has come upon him again more the hard things also so made that hard fast that he knew there would be too much to hurt her\n",
      "Epoch 351/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.4861 - acc: 0.4668\n",
      "Epoch 352/500\n",
      "43796/43796 [==============================] - 27s 625us/step - loss: 2.4761 - acc: 0.4643\n",
      "Epoch 353/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.4650 - acc: 0.4687\n",
      "Epoch 354/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.4669 - acc: 0.4657\n",
      "Epoch 355/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.4756 - acc: 0.4648\n",
      "Epoch 356/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.4693 - acc: 0.4661\n",
      "Epoch 357/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.4665 - acc: 0.4677\n",
      "Epoch 358/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.4677 - acc: 0.4670\n",
      "Epoch 359/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.4658 - acc: 0.4671\n",
      "Epoch 360/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.4598 - acc: 0.4703\n",
      "Epoch 361/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.4679 - acc: 0.4690\n",
      "Epoch 362/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.4650 - acc: 0.4661\n",
      "Epoch 363/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.4605 - acc: 0.4687\n",
      "Epoch 364/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.4609 - acc: 0.4660\n",
      "Epoch 365/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.4491 - acc: 0.4700\n",
      "Epoch 366/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.4541 - acc: 0.4702\n",
      "Epoch 367/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.4492 - acc: 0.4692\n",
      "Epoch 368/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.4565 - acc: 0.4694\n",
      "Epoch 369/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.4399 - acc: 0.4742\n",
      "Epoch 370/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.4402 - acc: 0.4732\n",
      "Epoch 371/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.4353 - acc: 0.4722\n",
      "Epoch 372/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.4451 - acc: 0.4704\n",
      "Epoch 373/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.4383 - acc: 0.4720\n",
      "Epoch 374/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.4389 - acc: 0.4726\n",
      "Epoch 375/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.4320 - acc: 0.4744\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      ", said dorothy . when they walked upon my head the travelers had quite only no , for he would make us him and give him of day , and now they were never ever mind him for many years , although he was in something about the wicked witch morning . then they walked out with her , where she went to be down . then he found another time on this journey he was quite angry , and\n",
      "Epoch 376/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.4306 - acc: 0.4734\n",
      "Epoch 377/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.4257 - acc: 0.4750\n",
      "Epoch 378/500\n",
      "43796/43796 [==============================] - 27s 621us/step - loss: 2.4270 - acc: 0.4717\n",
      "Epoch 379/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.4311 - acc: 0.4722\n",
      "Epoch 380/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.4292 - acc: 0.4727\n",
      "Epoch 381/500\n",
      "43796/43796 [==============================] - 27s 625us/step - loss: 2.4278 - acc: 0.4726\n",
      "Epoch 382/500\n",
      "43796/43796 [==============================] - 28s 635us/step - loss: 2.4266 - acc: 0.4742\n",
      "Epoch 383/500\n",
      "43796/43796 [==============================] - 27s 625us/step - loss: 2.4233 - acc: 0.4751\n",
      "Epoch 384/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.4077 - acc: 0.4771\n",
      "Epoch 385/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.4165 - acc: 0.4731\n",
      "Epoch 386/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.4138 - acc: 0.4742\n",
      "Epoch 387/500\n",
      "43796/43796 [==============================] - 27s 625us/step - loss: 2.4097 - acc: 0.4781\n",
      "Epoch 388/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.4092 - acc: 0.4764\n",
      "Epoch 389/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.3977 - acc: 0.4781\n",
      "Epoch 390/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.4083 - acc: 0.4776\n",
      "Epoch 391/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.4108 - acc: 0.4764\n",
      "Epoch 392/500\n",
      "43796/43796 [==============================] - 27s 625us/step - loss: 2.4046 - acc: 0.4760\n",
      "Epoch 393/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.4042 - acc: 0.4780\n",
      "Epoch 394/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.4048 - acc: 0.4737\n",
      "Epoch 395/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.4013 - acc: 0.4768\n",
      "Epoch 396/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.3962 - acc: 0.4781\n",
      "Epoch 397/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3965 - acc: 0.4806\n",
      "Epoch 398/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.3866 - acc: 0.4816\n",
      "Epoch 399/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.3948 - acc: 0.4755\n",
      "Epoch 400/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3869 - acc: 0.4803\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "is just on any place he went out of his forehead . here she had always ever be afraid . indeed , they started along beside him . said the scarecrow , who can be good brains but i shall go with my heart that it are like on all that any first is be , said the tin woodman . because that is why we cannot come with me , yes , answered the little man . just as\n",
      "Epoch 401/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3922 - acc: 0.4789\n",
      "Epoch 402/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3847 - acc: 0.4808\n",
      "Epoch 403/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3827 - acc: 0.4792\n",
      "Epoch 404/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3821 - acc: 0.4823\n",
      "Epoch 405/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.3793 - acc: 0.4793\n",
      "Epoch 406/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.3803 - acc: 0.4821\n",
      "Epoch 407/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3719 - acc: 0.4825\n",
      "Epoch 408/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.3683 - acc: 0.4801\n",
      "Epoch 409/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.3688 - acc: 0.4808\n",
      "Epoch 410/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.3609 - acc: 0.4847\n",
      "Epoch 411/500\n",
      "43796/43796 [==============================] - 27s 625us/step - loss: 2.3634 - acc: 0.4839\n",
      "Epoch 412/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.3643 - acc: 0.4825\n",
      "Epoch 413/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.3651 - acc: 0.4818\n",
      "Epoch 414/500\n",
      "43796/43796 [==============================] - 27s 625us/step - loss: 2.3644 - acc: 0.4828\n",
      "Epoch 415/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3599 - acc: 0.4861\n",
      "Epoch 416/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.3657 - acc: 0.4828\n",
      "Epoch 417/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3564 - acc: 0.4827\n",
      "Epoch 418/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.3567 - acc: 0.4858\n",
      "Epoch 419/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.3521 - acc: 0.4855\n",
      "Epoch 420/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.3495 - acc: 0.4863\n",
      "Epoch 421/500\n",
      "43796/43796 [==============================] - 27s 621us/step - loss: 2.3548 - acc: 0.4840\n",
      "Epoch 422/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3394 - acc: 0.4868\n",
      "Epoch 423/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3482 - acc: 0.4834\n",
      "Epoch 424/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.3409 - acc: 0.4853\n",
      "Epoch 425/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.3475 - acc: 0.4875\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "looking at once without the other travelers she did come over , where dorothy was ready . but the next morning the soldier had toto and started back to make his way to sight , so that told them hard at the winged monkeys came being down , and at last came back to see the house full of its silk like his body , and he told her in the world she seemed up to do next , replied\n",
      "Epoch 426/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.3360 - acc: 0.4872\n",
      "Epoch 427/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.3453 - acc: 0.4871\n",
      "Epoch 428/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3380 - acc: 0.4899\n",
      "Epoch 429/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.3407 - acc: 0.4865\n",
      "Epoch 430/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3437 - acc: 0.4859\n",
      "Epoch 431/500\n",
      "43796/43796 [==============================] - 27s 625us/step - loss: 2.3354 - acc: 0.4900\n",
      "Epoch 432/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3290 - acc: 0.4885\n",
      "Epoch 433/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3173 - acc: 0.4888\n",
      "Epoch 434/500\n",
      "43796/43796 [==============================] - 27s 621us/step - loss: 2.3373 - acc: 0.4886\n",
      "Epoch 435/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3282 - acc: 0.4897\n",
      "Epoch 436/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3287 - acc: 0.4918\n",
      "Epoch 437/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.3163 - acc: 0.4910\n",
      "Epoch 438/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3236 - acc: 0.4870\n",
      "Epoch 439/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.3184 - acc: 0.4907\n",
      "Epoch 440/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.3155 - acc: 0.4907\n",
      "Epoch 441/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3233 - acc: 0.4903\n",
      "Epoch 442/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3132 - acc: 0.4916\n",
      "Epoch 443/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3131 - acc: 0.4918\n",
      "Epoch 444/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3130 - acc: 0.4905\n",
      "Epoch 445/500\n",
      "43796/43796 [==============================] - 27s 625us/step - loss: 2.3144 - acc: 0.4891\n",
      "Epoch 446/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3128 - acc: 0.4909\n",
      "Epoch 447/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3125 - acc: 0.4908\n",
      "Epoch 448/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3089 - acc: 0.4921\n",
      "Epoch 449/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3030 - acc: 0.4906\n",
      "Epoch 450/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.2970 - acc: 0.4935\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "the trees and began to work where she had been as well , and then he stood out at their long back that cried the cowardly lion , we cannot take from once , indeed . there is only wolves again . there is no longer my joints , for she asked . oh , yes oh , yes oh , yes that is a great very out of yourself and live , no , i shall have no heart\n",
      "Epoch 451/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.2870 - acc: 0.4937\n",
      "Epoch 452/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3060 - acc: 0.4932\n",
      "Epoch 453/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.2917 - acc: 0.4945\n",
      "Epoch 454/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.2947 - acc: 0.4932\n",
      "Epoch 455/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3010 - acc: 0.4919\n",
      "Epoch 456/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.2961 - acc: 0.4950\n",
      "Epoch 457/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.3043 - acc: 0.4926\n",
      "Epoch 458/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.2960 - acc: 0.4955\n",
      "Epoch 459/500\n",
      "43796/43796 [==============================] - 27s 621us/step - loss: 2.2927 - acc: 0.4918\n",
      "Epoch 460/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.2980 - acc: 0.4927\n",
      "Epoch 461/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.2801 - acc: 0.4952\n",
      "Epoch 462/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.2797 - acc: 0.4975\n",
      "Epoch 463/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.2802 - acc: 0.4991\n",
      "Epoch 464/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.2825 - acc: 0.4963\n",
      "Epoch 465/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.2866 - acc: 0.4956\n",
      "Epoch 466/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.2794 - acc: 0.4986\n",
      "Epoch 467/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.2810 - acc: 0.4966\n",
      "Epoch 468/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.2683 - acc: 0.4957\n",
      "Epoch 469/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.2857 - acc: 0.4948\n",
      "Epoch 470/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.2734 - acc: 0.4968\n",
      "Epoch 471/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.2659 - acc: 0.4969\n",
      "Epoch 472/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.2640 - acc: 0.4997\n",
      "Epoch 473/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.2735 - acc: 0.4975\n",
      "Epoch 474/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.2664 - acc: 0.4995\n",
      "Epoch 475/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.2595 - acc: 0.5003\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "he will . when they dont go into the throne room good bye so he thought he could not come back , and when she had no people nor of a beautiful creature that they walked across and these people nothing in her . the winged monkeys began to get into the road , where they saw that soon could not let him , for first they started into the forest . these people being well for oz but the\n",
      "Epoch 476/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.2683 - acc: 0.4980\n",
      "Epoch 477/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.2611 - acc: 0.5005\n",
      "Epoch 478/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.2485 - acc: 0.5012\n",
      "Epoch 479/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.2624 - acc: 0.4997\n",
      "Epoch 480/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.2552 - acc: 0.5001\n",
      "Epoch 481/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.2497 - acc: 0.5016\n",
      "Epoch 482/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.2607 - acc: 0.4984\n",
      "Epoch 483/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.2675 - acc: 0.4992\n",
      "Epoch 484/500\n",
      "43796/43796 [==============================] - 27s 625us/step - loss: 2.2504 - acc: 0.5021\n",
      "Epoch 485/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.2536 - acc: 0.5009\n",
      "Epoch 486/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.2493 - acc: 0.5018\n",
      "Epoch 487/500\n",
      "43796/43796 [==============================] - 27s 621us/step - loss: 2.2430 - acc: 0.5047\n",
      "Epoch 488/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.2412 - acc: 0.5037\n",
      "Epoch 489/500\n",
      "43796/43796 [==============================] - 27s 625us/step - loss: 2.2498 - acc: 0.5016\n",
      "Epoch 490/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.2520 - acc: 0.5016\n",
      "Epoch 491/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.2458 - acc: 0.5026\n",
      "Epoch 492/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.2411 - acc: 0.5037\n",
      "Epoch 493/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.2264 - acc: 0.5022\n",
      "Epoch 494/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.2453 - acc: 0.5045\n",
      "Epoch 495/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.2428 - acc: 0.5007\n",
      "Epoch 496/500\n",
      "43796/43796 [==============================] - 27s 621us/step - loss: 2.2379 - acc: 0.5020\n",
      "Epoch 497/500\n",
      "43796/43796 [==============================] - 27s 624us/step - loss: 2.2308 - acc: 0.5037\n",
      "Epoch 498/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.2355 - acc: 0.5035\n",
      "Epoch 499/500\n",
      "43796/43796 [==============================] - 27s 623us/step - loss: 2.2254 - acc: 0.5066\n",
      "Epoch 500/500\n",
      "43796/43796 [==============================] - 27s 622us/step - loss: 2.2261 - acc: 0.5049\n",
      "Checkpointing the model...\n",
      "Generating Text...\n",
      "a strange roar and which she called her could be at once , so that dorothy walked near , dorothy could see how any had made in an pretty rooms than home , and after toto and ever would run away from his beautiful country . dorothy told his years being well , while he said where the winged monkeys was stuffed with straw , even three one so one that the first old place before him , for he\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f96676bbcc0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, batch_size=256, epochs=500, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I am still trying to export this model into Javascript, I also need to be able to replicate the tokenizer. To do that, I take the tokenizer and construct an array. Every word is the placed in the array at the index which is the number the Tokenizer associates that word with. (Given 1-indexing). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25804"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "js = \"[\"\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    js += '\"' + word + '\",'\n",
    "js += \"]\"\n",
    "f = open(TEXT_NAME + \"-js-tokenizer.txt\", \"w\")\n",
    "f.write(js)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Models and the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load cleaned text sequences\n",
    "in_filename = 'harry-potter.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "seq_length = len(lines[0].split()) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from pickle import load\n",
    "\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text\n",
    "\n",
    "Before you generate the text. we first need to have a model loaded, which obviously we already do, since we just finished training it. Then, the generation algorithm is as follows.\n",
    "\n",
    "1. Randomly choose a line of text to be the seed text\n",
    "2. Encode the words into numbers using the Tokenizer\n",
    "3. Ensure that the Encoded seed text is the right length\n",
    "4. Have the model predict the next class to code\n",
    "5. Convert the number back into word format\n",
    "6. Append the new word to the seed text\n",
    "7. Repeat with the newly generated word in the seed text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arms around the trolls neck from behind the troll couldnt feel harry hanging there but even a troll will notice if you stick a long bit of wood up its nose and harrys wand had still been in his hand when hed jumped it had gone straight up one of the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "def generate_text():\n",
    "    \n",
    "    result = list()\n",
    "    # select a seed text\n",
    "    seed_text = lines[randint(0,len(lines))]\n",
    "    \n",
    "    for i in range(words_to_generate):\n",
    "        # encode the seed text\n",
    "        encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "\n",
    "        # append to input\n",
    "        seed_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['penis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
