{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import string\n",
    "from numpy import array\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from pickle import load\n",
    "from pickle import dump\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from random import randint\n",
    "import sys\n",
    "import io, getopt, ast\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================================================================\n",
    "# parameters\n",
    "\n",
    "# This is the file with the pre separated lines of 51 words\n",
    "dataset_path = \"./harry-potter.txt\"\n",
    "\n",
    "load_file = False\n",
    "load_path = \"./checkpoint.h5\"\n",
    "save_path = \"./\"\n",
    "file_name = \"checkpoint\"\n",
    "\n",
    "num_epochs = 5\n",
    "checkpoints = list(range(num_epochs))\n",
    "batch_size = 256\n",
    "step_size = 1\n",
    "words_to_generate = 60\n",
    "\n",
    "input_size = 50\n",
    "output_size = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-model.py -d <dataset> -c <epochs to checkpoint> -e <# epochs> -l <model to load from> -n <name of files>\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/TomCat/Github-Repos/lstm-text-gen/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3273: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================================================\n",
    "# Takes Command line inputs to override the above\n",
    "if __name__ == \"__main__\":\n",
    "    argv = sys.argv[1:]\n",
    "\n",
    "    try:\n",
    "        opts, args = getopt.getopt(argv,\"hd:c:e:l:n:\",[\"dataset=\",\"checkpoints=\",\"epochs=\",\"load_model=\",\"name=\"])\n",
    "    except getopt.GetoptError:\n",
    "        print ('train-model.py -d <dataset> -c <epochs to checkpoint> -e <# epochs> -l <model to load from> -n <name of files>')\n",
    "        sys.exit(2)\n",
    "\n",
    "    for opt, arg in opts:\n",
    "\n",
    "        # Help Command\n",
    "        if opt == '-h':\n",
    "            print ('train-model.py -d <dataset> -c <epochs to checkpoint> -e <# epochs> -l <model to load from> -n <name of files>')\n",
    "            sys.exit()\n",
    "\n",
    "        # Num Epochs\n",
    "        elif opt in (\"-e\",\"--epochs\"):\n",
    "            num_epochs = ast.literal_eval(arg)\n",
    "            checkpoints = list(range(num_epochs))\n",
    "\n",
    "        # Dataset Name\n",
    "        elif opt in (\"-d\", \"--dataset\"):\n",
    "            dataset_path = arg\n",
    "\n",
    "        # Checkpoints\n",
    "        elif opt in (\"-c\", \"--checkpoints\"):\n",
    "            checkpoints = ast.literal_eval(arg)\n",
    "\n",
    "        elif opt in (\"-n\", \"--name\"):\n",
    "            file_name = arg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================================================================\n",
    "# Load The Model\n",
    "\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================================================================\n",
    "# Load the Dataset with the lines of text\n",
    "\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "doc = load_doc(dataset_path)\n",
    "lines = doc.split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================================================================\n",
    "# Use the tokenizer we just loaded to prepare the sequences we're using\n",
    "\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 5868\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation from each token\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocab Size: %d\" % vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================================================================\n",
    "# Define function to define the generated text\n",
    "\n",
    "def generate_text():\n",
    "    \n",
    "    result = list()\n",
    "    # select a seed text\n",
    "    seed_text = lines[randint(0,len(lines))]\n",
    "    \n",
    "    for i in range(words_to_generate):\n",
    "        # encode the seed text\n",
    "        encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "\n",
    "        # append to input\n",
    "        seed_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================================================================\n",
    "# Define the Callback Function\n",
    "\n",
    "def on_epoch_end (epoch, _):\n",
    "    \n",
    "    # Checkpointing the model\n",
    "    for i in checkpoints:\n",
    "        if epoch + 1 == i:\n",
    "            print(\"Checkpointing the model...\")\n",
    "            model.save(\"%s.h5\" % (file_name))\n",
    "    print(\"Generating Text...\")\n",
    "    print(generate_text())\n",
    "    \n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================================================================\n",
    "# Fit Model\n",
    "\n",
    "model.fit(X, y, batch_size=batch_size, epochs=num_epochs, callbacks=[print_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
