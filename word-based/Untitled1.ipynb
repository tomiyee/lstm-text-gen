{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "from pathlib import Path\n",
    "\n",
    "# parameters\n",
    "dataset = \"./datasets/harry-potter-1-2-4.txt\"\n",
    "load_path = \"./checkpoint.h5\"\n",
    "sample_length = 800\n",
    "\n",
    "\n",
    "\n",
    "# Reads the Files and stuff\n",
    "with io.open(dataset, encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "\n",
    "\n",
    "my_file = Path()\n",
    "if my_file.is_file():\n",
    "    print(\"Found Checkpoint. Loading Saved model\")\n",
    "    model = load_model(load_path)\n",
    "else:\n",
    "    print(\"NO MODEL FOUND\")\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "# Generates the Text\n",
    "print()\n",
    "\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "    print('\\n----- diversity:', diversity)\n",
    "\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(sample_length):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation.replace('.','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 3 '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" {} \".format(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"3\" in \"123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import string\n",
    "from numpy import array\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from pickle import load\n",
    "from pickle import dump\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from random import randint\n",
    "import sys\n",
    "import io, getopt, ast\n",
    "from pathlib import Path\n",
    "\n",
    "# ===================================================================\n",
    "# parameters\n",
    "\n",
    "# Path to the raw corpus\n",
    "raw_corpus =\"../datasets/harry-potter-1.txt\"\n",
    "# This is the file with the pre separated lines of 51 words\n",
    "dataset_path = \"./harry-potter.txt\"\n",
    "\n",
    "load_existing_model = False\n",
    "filename  = \"generated-model\"\n",
    "load_path = \"./\" + filename + \".h5\"\n",
    "save_path = \"./\"\n",
    "\n",
    "num_epochs = 5\n",
    "checkpoints = list(range(1,num_epochs+1))\n",
    "batch_size = 256\n",
    "words_to_generate = 60\n",
    "\n",
    "input_size = 50\n",
    "output_size = 1\n",
    "\n",
    "have_period = True\n",
    "# ===================================================================\n",
    "# Takes Command line inputs to override the above\n",
    "if __name__ == \"__main__\":\n",
    "    argv = sys.argv[1:]\n",
    "\n",
    "    try:\n",
    "        opts, args = getopt.getopt(argv,\"hd:c:e:l:n:\",[\"dataset=\",\"checkpoints=\",\"epochs=\",\"load_model=\",\"name=\"])\n",
    "    except getopt.GetoptError:\n",
    "        print ('train-model.py -d <dataset> -c <epochs to checkpoint> -e <# epochs> -l <model to load from> -n <name of files>')\n",
    "        sys.exit(2)\n",
    "\n",
    "    for opt, arg in opts:\n",
    "\n",
    "        # Help Command\n",
    "        if opt == '-h':\n",
    "            print ('train-model.py -d <dataset> -c <epochs to checkpoint> -e <# epochs> -l <model to load from> -n <name of files>')\n",
    "            sys.exit()\n",
    "\n",
    "        # Num Epochs\n",
    "        elif opt in (\"-e\",\"--epochs\"):\n",
    "            num_epochs = ast.literal_eval(arg)\n",
    "            checkpoints = list(range(num_epochs))\n",
    "\n",
    "        # Load Model\n",
    "        elif opt in (\"-l\", \"--load_model\"):\n",
    "            load_existing_model = True\n",
    "            filename = arg\n",
    "            \n",
    "        # Name of the file with the raw corpus\n",
    "        elif opt in (\"-d\", \"--dataset\"):\n",
    "            raw_corpus = \"../datasets/\" + arg\n",
    "\n",
    "        # Checkpoints\n",
    "        elif opt in (\"-c\", \"--checkpoints\"):\n",
    "            checkpoints = ast.literal_eval(arg)\n",
    "\n",
    "        elif opt in (\"-n\", \"--name\"):\n",
    "            filename = arg\n",
    "# ===================================================================\n",
    "# Load The Model or create a new model\n",
    "if load_existing_model:\n",
    "    print(\"Loading Existing Model...\")\n",
    "    # load the model\n",
    "    model = load_model(filename + '.h5')\n",
    "    # load the tokenizer\n",
    "    tokenizer = load(open(filename + '-tokenizer.pkl', 'rb'))\n",
    "else:\n",
    "    print(\"Generating New Model...\")\n",
    "    \n",
    "    # =================================================\n",
    "    # Dataset Acquisition\n",
    "    \n",
    "    # loads doc into memory\n",
    "    def load_doc(filename):\n",
    "        # open the file as read only\n",
    "        file = open(filename, 'r')\n",
    "        # read all text\n",
    "        text = file.read()\n",
    "        # close the file\n",
    "        file.close()\n",
    "        return text\n",
    "\n",
    "    # turns a doc into clean tokens\n",
    "    def clean_doc(doc):\n",
    "        # make lower case\n",
    "        doc = doc.lower()\n",
    "        # replace '--' with a space ' '\n",
    "        doc = doc.replace('--', ' ')\n",
    "        doc = doc.replace('-', ' ')\n",
    "        if have_period:\n",
    "            doc = doc.replace('. ', ' . ')\n",
    "        # split into tokens by white space\n",
    "        tokens = doc.split()\n",
    "        punctuation = string.punctuation\n",
    "        if have_period:\n",
    "            punctuation = punctuation.replace('.','')\n",
    "        # remove punctuation from each token\n",
    "        table = str.maketrans('', '', punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        tokens = [word for word in tokens if word.isalpha() or word is '.']\n",
    "        return tokens\n",
    "\n",
    "    # load document\n",
    "    doc = load_doc(raw_corpus)\n",
    "    # clean document\n",
    "    tokens = clean_doc(doc)\n",
    "    print(tokens[:20])\n",
    "    print('Total Tokens: %d' % len(tokens))\n",
    "    print('Unique Tokens: %d' % len(set(tokens)))\n",
    "    \n",
    "    # =================================================\n",
    "    # Dataset Preparation and Preservation\n",
    "\n",
    "    # organize into sequences of tokens\n",
    "    length = input_size + output_size\n",
    "    sequences = list()\n",
    "    for i in range(length, len(tokens)):\n",
    "        # select sequence of tokens\n",
    "        seq = tokens[i-length:i]\n",
    "        # convert into a line\n",
    "        line = ' '.join(seq)\n",
    "        # store\n",
    "        sequences.append(line)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "    # save tokens to file, one dialog per line\n",
    "    def save_doc(lines, filename):\n",
    "        data = '\\n'.join(lines)\n",
    "        file = open(filename, 'w')\n",
    "        file.write(data)\n",
    "        file.close()\n",
    "\n",
    "    # save sequences to file\n",
    "    save_doc(sequences, filename + \"-lines.txt\")\n",
    "    \n",
    "    # =================================================\n",
    "    # Tokenize Lines, Vocab Size Determination\n",
    "    \n",
    "    # load doc into memory\n",
    "    def load_doc(filename):\n",
    "        # open the file as read only\n",
    "        file = open(filename, 'r')\n",
    "        # read all text\n",
    "        text = file.read()\n",
    "        # close the file\n",
    "        file.close()\n",
    "        return text\n",
    "\n",
    "    # load\n",
    "    doc = load_doc(filename + \"-lines.txt\")\n",
    "    lines = doc.split('\\n')\n",
    "    \n",
    "    # integer encode sequences of words\n",
    "    punctuation = string.punctuation\n",
    "    if have_period:\n",
    "        punctuation = punctuation.replace('.','')\n",
    "    tokenizer = Tokenizer(filters=punctuation)\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    sequences = tokenizer.texts_to_sequences(lines)\n",
    "    # save the tokenizer\n",
    "    dump(tokenizer, open(filename + '-tokenizer.pkl', 'wb'))\n",
    "    \n",
    "    # vocabulary size\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print(vocab_size)\n",
    "    \n",
    "    # separate into input and output\n",
    "    sequences = array(sequences)\n",
    "    X, y = sequences[:,:-1], sequences[:,-1]\n",
    "    y = to_categorical(y, num_classes=vocab_size)\n",
    "    seq_length = X.shape[1]\n",
    "    \n",
    "    # =================================================\n",
    "    # Model Creation\n",
    "\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, input_size, input_length=seq_length))\n",
    "    model.add(LSTM(96, return_sequences=True))\n",
    "    model.add(LSTM(96))\n",
    "    model.add(Dense(96, activation='relu'))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "# ===================================================================\n",
    "# Load the Dataset with the lines of text\n",
    "\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "doc = load_doc(filename + \"-lines.txt\")\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "# ===================================================================\n",
    "# Use the tokenizer we just loaded to prepare the sequences we're using\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open(filename + '-tokenizer.pkl', 'rb'))\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "\n",
    "# remove punctuation from each token\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocab Size: %d\" % vocab_size)\n",
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "\n",
    "# ===================================================================\n",
    "# Define function to define the generated text\n",
    "\n",
    "def generate_text():\n",
    "    \n",
    "    result = list()\n",
    "    # select a seed text\n",
    "    seed_text = lines[randint(0,len(lines))]\n",
    "    \n",
    "    for i in range(words_to_generate):\n",
    "        # encode the seed text\n",
    "        encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "\n",
    "        # append to input\n",
    "        seed_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "# ===================================================================\n",
    "# Define the Callback Function\n",
    "\n",
    "def on_epoch_end (epoch, _):\n",
    "    \n",
    "    # Checkpointing the model\n",
    "    for i in checkpoints:\n",
    "        if epoch + 1 == i:\n",
    "            print(\"Checkpointing the model...\")\n",
    "            model.save(\"%s-cp-%d.h5\" % (filename, i))\n",
    "            break\n",
    "    print(\"Generating Text...\")\n",
    "    print(generate_text())\n",
    "    \n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "# ===================================================================\n",
    "# Fit Model\n",
    "\n",
    "model.fit(X, y, batch_size=batch_size, epochs=num_epochs, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'.' in \".,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitelisted_punctuation = \".,\"\n",
    "\n",
    "doc = \"Tom. Heng, \"\n",
    "\n",
    "for char in whitelisted_punctuation:\n",
    "    doc = doc.replace('{} '.format(char), ' {} '.format(char))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tom . Heng , '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
